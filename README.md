# CODESIM ë¦¬ë·° (NAACL 2025 Findings)

## ğŸ“‹ ë…¼ë¬¸ ì»¨í…ìŠ¤íŠ¸

**CODESIM**ì€ NAACL 2025 Findingsì—ì„œ ì œì‹œëœ í˜ì‹ ì ì¸ multi-agent code generation í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ê¸°ì¡´ MapCoderì˜ "multiple ungrounded exemplars" ì ‘ê·¼ë²•ì„ ê°œì„ í•˜ì—¬ **"single exemplar" ê¸°ë°˜ì˜ simulation-driven planning and debugging**ì„ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤.

### ğŸš€ í•µì‹¬ í˜ì‹ 
- **3-Agent Architecture**: Planning Agent, Coding Agent, Debugging Agentì˜ í˜‘ë ¥ì  êµ¬ì¡°
- **Simulation-Driven Verification**: Step-by-step ì‹œë®¬ë ˆì´ì…˜ì„ í†µí•œ ê³„íš ê²€ì¦
- **Internal Debugging**: ì™¸ë¶€ ë„êµ¬ ì—†ì´ ì‹œë®¬ë ˆì´ì…˜ ê¸°ë°˜ ë‚´ë¶€ ë””ë²„ê¹…
- **Human-like Perception**: ì¸ê°„ì˜ ì•Œê³ ë¦¬ì¦˜ ì‹œê°ì  ê²€ì¦ ë°©ì‹ êµ¬í˜„

### ğŸ“Š SOTA ì„±ëŠ¥ ë‹¬ì„±
- **HumanEval**: 95.1% (Pass@1)
- **MBPP**: 90.7% (Pass@1)  
- **APPS**: 22% (Pass@1)
- **CodeContests**: 29.1% (Pass@1)

## ğŸ—ï¸ ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Planning Agent â”‚â”€â”€â”€â–¶â”‚  Coding Agent   â”‚â”€â”€â”€â–¶â”‚ Debugging Agent â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ Problem       â”‚    â”‚ â€¢ Plan â†’ Code   â”‚    â”‚ â€¢ Internal      â”‚
â”‚   Understanding â”‚    â”‚ â€¢ Execution     â”‚    â”‚   Simulation    â”‚
â”‚ â€¢ Exemplar      â”‚    â”‚ â€¢ Generation    â”‚    â”‚ â€¢ Bug Detection â”‚
â”‚   Recall        â”‚    â”‚                 â”‚    â”‚ â€¢ Code Fix      â”‚
â”‚ â€¢ Algorithm     â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚   Design        â”‚    â”‚                 â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Plan Simulation â”‚    â”‚ Code Execution  â”‚    â”‚ Test Validation â”‚
â”‚ & Verification  â”‚    â”‚ & Evaluation    â”‚    â”‚ & Refinement    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ” í•µì‹¬ êµ¬í˜„ ë¶„ì„

### 1. 3-Agent êµ¬ì¡° êµ¬í˜„

#### Planning Agent (`CodeSIM.py` lines 200-280)
```python
# í•µì‹¬ ê³„íš ìƒì„± ë¡œì§
input_for_planning = [
    {
        "role": "user", 
        "content": prompt_for_planning.format(
            problem=problem,
            language=self.language,
        )
    },
]

# ê³„íš êµ¬ì¡°í™”
if "### Plan" not in response:
    plan = f"### Plan\n\n{response}"
else:
    plan = response[response.rfind("### Plan"):]
```

**êµ¬í˜„ íŠ¹ì§•:**
- **Problem Understanding**: ë¬¸ì œ ìœ í˜• ë° ì œì•½ì‚¬í•­ ë¶„ì„
- **Exemplar Recall**: ê´€ë ¨ ì˜ˆì œ ë¬¸ì œ íšŒìƒ ë° ì•Œê³ ë¦¬ì¦˜ ë¶„ì„
- **Algorithm Design**: ìµœì  ì•Œê³ ë¦¬ì¦˜ ì„ íƒ ë° íŠœí† ë¦¬ì–¼ ì œê³µ
- **Step-by-step Planning**: ìƒì„¸í•œ ì‹¤í–‰ ê³„íš ìˆ˜ë¦½

#### Coding Agent (`CodeSIM.py` lines 320-350)
```python
# ê³„íš ê¸°ë°˜ ì½”ë“œ ìƒì„±
input_for_final_code_generation = [
    {
        "role": "user",
        "content": prompt_for_code_generation.format(
            problem_with_planning=problem_with_planning,
            language=self.language,
            std_input_prompt=std_input_prompt,
        )
    }
]

code = parse_response(response)
```

**êµ¬í˜„ íŠ¹ì§•:**
- **Plan-to-Code Translation**: ê²€ì¦ëœ ê³„íšì„ ì‹¤í–‰ ê°€ëŠ¥í•œ ì½”ë“œë¡œ ë³€í™˜
- **Language-Specific Generation**: í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë³„ ìµœì í™”ëœ ì½”ë“œ ìƒì„±
- **Standard I/O Handling**: ê²½ìŸ í”„ë¡œê·¸ë˜ë° í™˜ê²½ì— ìµœì í™”ëœ ì…ì¶œë ¥ ì²˜ë¦¬

#### Debugging Agent (`CodeSIM.py` lines 360-420)
```python
# ë‚´ë¶€ ë””ë²„ê¹… ë©”ì»¤ë‹ˆì¦˜
for debug_no in range(1, self.max_debug_try + 1):
    input_for_debugging = [
        {
            "role": "user",
            "content": prompt_for_debugging.format(
                problem_with_planning=problem_with_planning,
                code=code,
                language=self.language,
                test_log=test_log,
                std_input_prompt=std_input_prompt,
            )
        }
    ]
    
    code = parse_response(response)
    passed, test_log = self.check(data_row, additional_io, code)
    
    if passed:
        break
```

**êµ¬í˜„ íŠ¹ì§•:**
- **Internal Simulation**: ì™¸ë¶€ ë„êµ¬ ì—†ì´ ì‹œë®¬ë ˆì´ì…˜ ê¸°ë°˜ ë²„ê·¸ íƒì§€
- **Step-by-step Analysis**: ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ì˜ ë‹¨ê³„ë³„ ë¶„ì„
- **Plan-Code Alignment**: ê³„íšê³¼ ì½”ë“œ ê°„ ë¶ˆì¼ì¹˜ ì  ê²€ì¶œ
- **Iterative Refinement**: ìµœëŒ€ 5íšŒê¹Œì§€ ë°˜ë³µì  ì½”ë“œ ê°œì„ 

### 2. Simulation-Driven ì ‘ê·¼ë²• êµ¬í˜„

#### Plan Verification (`CodeSIM.py` lines 290-320)
```python
# ê³„íš ì‹œë®¬ë ˆì´ì…˜ ë° ê²€ì¦
input_for_simulation = [
    {
        "role": "user",
        "content": prompt_for_simulation.format(
            problem_with_planning=problem_with_planning,
            language=self.language,
        )
    },
]

# ê³„íš ìˆ˜ì • í•„ìš”ì„± íŒë‹¨
if "Plan Modification Needed" in response and \
    "No Plan Modification Needed" not in response:
    
    # ê³„íš ì •ì œ ë‹¨ê³„
    input_for_plan_refinement = [
        {
            "role": "user",
            "content": prompt_for_plan_refinement.format(
                problem_with_planning=problem_with_planning,
                language=self.language,
                critique=response
            )
        },
    ]
```

**êµ¬í˜„ íŠ¹ì§•:**
- **Manual Simulation**: ì½”ë“œ ì—†ì´ ìˆ˜ë™ìœ¼ë¡œ ê³„íš ë‹¨ê³„ë³„ ì‹¤í–‰
- **Output Comparison**: ì˜ˆìƒ ì¶œë ¥ê³¼ ì‹¤ì œ ì¶œë ¥ ë¹„êµ ê²€ì¦
- **Plan Critique**: ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ ê¸°ë°˜ ê³„íš ë¹„íŒì  ë¶„ì„
- **Iterative Refinement**: ìµœëŒ€ 5íšŒê¹Œì§€ ê³„íš ê°œì„  ë°˜ë³µ

#### Internal Debugging Simulation
```python
# ë””ë²„ê¹… í”„ë¡¬í”„íŠ¸ì˜ ì‹œë®¬ë ˆì´ì…˜ ì§€ì‹œì‚¬í•­
prompt_for_debugging = """
### Simulation with failed test case
To detect where is the bug follow following steps:
    - Take a sample test case where it fails.
    - Take the input go through each step according to the plan
    - You will get a output that must be different from the expected output.

### Debugging Notes
- Based on this simulation detect any of the following cases:
    - Plan is wrong
    - Plan is correct but plan to code generation is wrong.
- Finally, discuss how to correct this code.
"""
```

**êµ¬í˜„ íŠ¹ì§•:**
- **Step-by-step Execution**: ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ì˜ ë‹¨ê³„ë³„ ì‹¤í–‰ ì‹œë®¬ë ˆì´ì…˜
- **Plan-Code Mismatch Detection**: ê³„íšê³¼ ì½”ë“œ ê°„ ë¶ˆì¼ì¹˜ ì  ì‹ë³„
- **Root Cause Analysis**: ë²„ê·¸ì˜ ê·¼ë³¸ ì›ì¸ ë¶„ì„
- **Corrective Action Planning**: ìˆ˜ì • ë°©ì•ˆ ìˆ˜ë¦½

### 3. Multi-Agent ê°„ í†µì‹  ë° ë°ì´í„° íë¦„

#### Agent ê°„ ë°ì´í„° ì „ë‹¬ êµ¬ì¡°
```python
# Planning â†’ Coding â†’ Debugging ë°ì´í„° íë¦„
problem_with_planning = f"## Problem:\n{problem}\n\n{plan}"

# ê° ë‹¨ê³„ì—ì„œ ì´ì „ ë‹¨ê³„ì˜ ê²°ê³¼ë¥¼ ì…ë ¥ìœ¼ë¡œ í™œìš©
input_for_final_code_generation = [
    {
        "role": "user",
        "content": prompt_for_code_generation.format(
            problem_with_planning=problem_with_planning,  # ê³„íš í¬í•¨
            language=self.language,
            std_input_prompt=std_input_prompt,
        )
    }
]

input_for_debugging = [
    {
        "role": "user", 
        "content": prompt_for_debugging.format(
            problem_with_planning=problem_with_planning,  # ê³„íš + ì½”ë“œ
            code=code,
            language=self.language,
            test_log=test_log,
            std_input_prompt=std_input_prompt,
        )
    }
]
```

#### ë°˜ë³µì  ê°œì„  ë©”ì»¤ë‹ˆì¦˜
```python
# Planning ë°˜ë³µ (ìµœëŒ€ 5íšŒ)
for plan_no in range(1, self.max_plan_try + 1):
    # ... planning logic ...
    if passed:
        break

# Debugging ë°˜ë³µ (ìµœëŒ€ 5íšŒ)  
for debug_no in range(1, self.max_debug_try + 1):
    # ... debugging logic ...
    if passed:
        break
```

## ğŸ§ª ë²¤ì¹˜ë§ˆí¬ ë° í‰ê°€ ì‹œìŠ¤í…œ

### ExecEval ì—°ë™ êµ¬í˜„
```python
# ì½”ë“œ ì‹¤í–‰ ë° í‰ê°€
def check(self, data_row: dict, additional_io: List[str], code: str) -> bool:
    passed_sample, test_log_sample = self.data.evaluate_sample_io(
        data_row, code, self.language
    )
    
    passed_additional, test_log_additional = self.data.evaluate_additional_io(
        data_row[self.data.id_key], additional_io, code, self.language
    )
    
    return passed_sample & passed_additional, test_log
```

### Pass@1 í‰ê°€ ë©”íŠ¸ë¦­
```python
# ê²°ê³¼ ì§‘ê³„ ë° ìš”ì•½
gen_summary(RESULTS_PATH, SUMMARY_PATH)

# ET/EP ë°ì´í„°ì…‹ ìƒì„±
if "human" in DATASET.lower():
    generate_et_dataset_human(RESULTS_PATH, ET_RESULTS_PATH)
    gen_summary(ET_RESULTS_PATH, ET_SUMMARY_PATH)
```

## ğŸ”§ ì‹¤í–‰ ê°€ì´ë“œ

### ê¸°ë³¸ ì‹¤í–‰ ëª…ë ¹ì–´
```bash
# HumanEval ë°ì´í„°ì…‹ìœ¼ë¡œ CodeSIM ì‹¤í–‰
python src/main.py --dataset HumanEval --strategy CodeSIM --model ChatGPT

# MBPP ë°ì´í„°ì…‹ìœ¼ë¡œ CodeSIM ì‹¤í–‰  
python src/main.py --dataset MBPP --strategy CodeSIM --model ChatGPT

# APPS ë°ì´í„°ì…‹ìœ¼ë¡œ CodeSIM ì‹¤í–‰
python src/main.py --dataset APPS --strategy CodeSIM --model ChatGPT

# LiveCodeBench ë°ì´í„°ì…‹ìœ¼ë¡œ CodeSIM ì‹¤í–‰
python src/main.py --dataset LiveCodeBench --strategy CodeSIM --model ChatGPT
```

### ê³ ê¸‰ ì‹¤í–‰ ì˜µì…˜
```bash
# ê³„íš ì‹œë„ íšŸìˆ˜ ë° ë””ë²„ê¹… ì‹œë„ íšŸìˆ˜ ì¡°ì •
python src/main.py \
    --dataset HumanEval \
    --strategy CodeSIM \
    --model ChatGPT \
    --temperature 0 \
    --top_p 0.95 \
    --pass_at_k 1
```

### ëª¨ë¸ë³„ ì‹¤í–‰
```bash
# Gemini ëª¨ë¸ ì‚¬ìš©
python src/main.py --dataset HumanEval --strategy CodeSIM --model gemini-pro --model_provider Gemini

# Groq ëª¨ë¸ ì‚¬ìš©
python src/main.py --dataset HumanEval --strategy CodeSIM --model mixtral-8x7b-32768 --model_provider Groq

# Anthropic ëª¨ë¸ ì‚¬ìš©
python src/main.py --dataset HumanEval --strategy CodeSIM --model claude-3-sonnet-20240229 --model_provider Anthropic
```

## ğŸ“Š ì½”ë“œ êµ¬ì¡° ìƒì„¸ ë¶„ì„

### í•µì‹¬ í´ë˜ìŠ¤ êµ¬ì¡°
```
src/
â”œâ”€â”€ promptings/
â”‚   â”œâ”€â”€ CodeSIM.py              # ë©”ì¸ CodeSIM êµ¬í˜„
â”‚   â”œâ”€â”€ variations/
â”‚   â”‚   â”œâ”€â”€ CodeSIMWPVD.py     # With Plan Verification & Debugging
â”‚   â”‚   â”œâ”€â”€ CodeSIMWD.py        # With Debugging
â”‚   â”‚   â”œâ”€â”€ CodeSIMWPV.py       # With Plan Verification
â”‚   â”‚   â”œâ”€â”€ CodeSIMA.py         # Analogical variation
â”‚   â”‚   â””â”€â”€ CodeSIMC.py         # Competitive programming
â”‚   â”œâ”€â”€ Base.py                 # ê¸°ë³¸ ì „ëµ í´ë˜ìŠ¤
â”‚   â””â”€â”€ PromptingFactory.py     # ì „ëµ íŒ©í† ë¦¬
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ ModelFactory.py         # ëª¨ë¸ íŒ©í† ë¦¬
â”‚   â”œâ”€â”€ OpenAI.py               # OpenAI ëª¨ë¸ êµ¬í˜„
â”‚   â”œâ”€â”€ Gemini.py               # Gemini ëª¨ë¸ êµ¬í˜„
â”‚   â””â”€â”€ Anthropic.py            # Anthropic ëª¨ë¸ êµ¬í˜„
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ DatasetFactory.py       # ë°ì´í„°ì…‹ íŒ©í† ë¦¬
â”‚   â”œâ”€â”€ HumanEvalDataset.py     # HumanEval ë°ì´í„°ì…‹
â”‚   â”œâ”€â”€ MBPPDataset.py          # MBPP ë°ì´í„°ì…‹
â”‚   â”œâ”€â”€ APPSDataset.py          # APPS ë°ì´í„°ì…‹
â”‚   â””â”€â”€ LiveCodeBenchDataset.py # LiveCodeBench ë°ì´í„°ì…‹
â””â”€â”€ evaluations/
    â”œâ”€â”€ func_evaluate.py         # í•¨ìˆ˜ í‰ê°€ ì—”ì§„
    â””â”€â”€ executor_utils.py        # ì‹¤í–‰ ìœ í‹¸ë¦¬í‹°
```

### ì£¼ìš” í•¨ìˆ˜ ë¶„ì„

#### 1. `run_single_pass()` - ë©”ì¸ ì‹¤í–‰ ë¡œì§
```python
def run_single_pass(self, data_row: dict):
    # 1. ë¬¸ì œ ë¶„ì„ ë° ì¶”ê°€ I/O ìˆ˜ì§‘
    problem = self.data.get_prompt(data_row)
    additional_io = []
    
    # 2. Planning Phase (ìµœëŒ€ 5íšŒ)
    for plan_no in range(1, self.max_plan_try + 1):
        # ê³„íš ìƒì„± â†’ ì‹œë®¬ë ˆì´ì…˜ ê²€ì¦ â†’ ê³„íš ì •ì œ
        # ì½”ë“œ ìƒì„± â†’ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        
        # 3. Debugging Phase (ìµœëŒ€ 5íšŒ)
        for debug_no in range(1, self.max_debug_try + 1):
            # ë‚´ë¶€ ì‹œë®¬ë ˆì´ì…˜ â†’ ë²„ê·¸ íƒì§€ â†’ ì½”ë“œ ìˆ˜ì •
            # í…ŒìŠ¤íŠ¸ ì¬ì‹¤í–‰
            
        if passed:
            break
```

#### 2. `check()` - ì½”ë“œ ê²€ì¦ ë¡œì§
```python
def check(self, data_row: dict, additional_io: List[str], code: str) -> bool:
    # ìƒ˜í”Œ I/O í‰ê°€
    passed_sample, test_log_sample = self.data.evaluate_sample_io(
        data_row, code, self.language
    )
    
    # ì¶”ê°€ I/O í‰ê°€  
    passed_additional, test_log_additional = self.data.evaluate_additional_io(
        data_row[self.data.id_key], additional_io, code, self.language
    )
    
    # í†µí•© ê²°ê³¼ ë°˜í™˜
    return passed_sample & passed_additional, test_log
```

## ğŸš€ ì„±ëŠ¥ ìµœì í™” êµ¬í˜„

### 1. ê²½ìŸ í”„ë¡œê·¸ë˜ë° ìµœì í™”
```python
# APPS, CodeContest, XCode ë°ì´í„°ì…‹ ìµœì í™”
self.is_competitive = type(self.data) == APPSDataset or \
    type(self.data) == CodeContestDataset or \
    type(self.data) == XCodeDataset

if self.is_competitive:
    std_input_prompt = """
    - Strictly follow the sample input and output format. 
    - The input should be taken from Standard input and output should be given to standard output.
    - For array input parse the array then pass it to the function.
    - Do not add extra print statement otherwise it will failed the test cases.
    """
```

### 2. LiveCodeBench ì „ìš© ìµœì í™”
```python
# LiveCodeBench ë°ì´í„°ì…‹ ê°ì§€
def is_livecodebench(self) -> bool:
    return self.dataset_type == 'livecodebench'

# LiveCodeBench ì „ìš© ê³„íš í”„ë¡¬í”„íŠ¸
if self.is_livecodebench():
    input_for_planning = [
        {
            "role": "user",
            "content": f"""You are a competitive programming expert tasked with generating an appropriate plan to solve a given LiveCodeBench problem using the **{self.language}** programming language.
            
            ## Problem
            {problem}
            
            **Expected Output:**
            Your response must be structured as follows:
            
            ### Problem Understanding
            - Think about the original problem. Develop an initial understanding about the problem.
            - Identify the problem type (array, string, graph, dynamic programming, etc.)
            - Note any constraints or edge cases
            
            ### Recall Example Problem
            Recall a relevant and distinct competitive programming problem (different from problem mentioned above) and
            - Describe it briefly
            - Identify the algorithm category (greedy, DP, graph, etc.)
            - Generate {self.language} code step by step to solve that problem
            - Discuss the algorithm to solve this problem
            - Finally generate a planning to solve that problem
            
            ### Algorithm to solve the original problem
            - Write down the algorithm that is well suited for the original problem
            - Give some tutorials about the algorithm for example:
                - How to approach this type of algorithm
                - Important things to consider
                - Time and space complexity analysis
            
            ### Plan
            - Write down a detailed, step-by-step plan to solve the **original problem**.
            - Include edge case handling
            - Consider optimization strategies
            
            --------
            **Important Instruction:**
            - Strictly follow the instructions.
            - Do not generate code.
            - Focus on competitive programming best practices."""
        },
    ]
```

## ğŸ”„ MapCoderì™€ì˜ ì°¨ì´ì 

### 1. Exemplar ì ‘ê·¼ë²• ì°¨ì´
- **MapCoder**: "multiple ungrounded exemplars" ì‚¬ìš©
- **CodeSIM**: "single exemplar" ê¸°ë°˜ ê³„íš ìƒì„±

### 2. ê³„íš ê²€ì¦ ë‹¨ê³„ ì¶”ê°€
```python
# CodeSIMì˜ ê³„íš ê²€ì¦ ë‹¨ê³„ (MapCoderì—ëŠ” ì—†ìŒ)
input_for_simulation = [
    {
        "role": "user",
        "content": prompt_for_simulation.format(
            problem_with_planning=problem_with_planning,
            language=self.language,
        )
    },
]

# ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ì— ë”°ë¥¸ ê³„íš ìˆ˜ì •
if "Plan Modification Needed" in response:
    input_for_plan_refinement = [
        {
            "role": "user",
            "content": prompt_for_plan_refinement.format(
                problem_with_planning=problem_with_planning,
                language=self.language,
                critique=response
            )
        },
    ]
```

### 3. ë‚´ë¶€ ë””ë²„ê¹… ë©”ì»¤ë‹ˆì¦˜
```python
# CodeSIMì˜ ì‹œë®¬ë ˆì´ì…˜ ê¸°ë°˜ ë‚´ë¶€ ë””ë²„ê¹…
prompt_for_debugging = """
### Simulation with failed test case
To detect where is the bug follow following steps:
    - Take a sample test case where it fails.
    - Take the input go through each step according to the plan
    - You will get a output that must be different from the expected output.

### Debugging Notes
- Based on this simulation detect any of the following cases:
    - Plan is wrong
    - Plan is correct but plan to code generation is wrong.
- Finally, discuss how to correct this code.
"""
```

## ğŸŒŸ í™•ì¥ì„± ë° ëª¨ë¸ ì—°ë™

### 1. ëª¨ë¸ íŒ©í† ë¦¬ íŒ¨í„´
```python
class ModelFactory:
    @staticmethod
    def get_model_class(model_provider_name: str):
        model_provider_name = model_provider_name.lower()
        if model_provider_name == "gemini":
            return Gemini
        elif model_provider_name == "openai":
            return OpenAIV1Model
        elif model_provider_name == "openai-v2":
            return OpenAIV2Model
        elif model_provider_name == "groq":
            return GroqModel
        elif model_provider_name == "anthropic":
            return AnthropicModel
```

### 2. ì „ëµ íŒ©í† ë¦¬ íŒ¨í„´
```python
class PromptingFactory:
    @staticmethod
    def get_prompting_class(strategy_name: str):
        if strategy_name == "CodeSIM":
            return CodeSIM
        elif strategy_name == "CodeSIMWPVD":
            return CodeSIMWPVD
        elif strategy_name == "CodeSIMWD":
            return CodeSIMWD
        elif strategy_name == "CodeSIMWPV":
            return CodeSIMWPV
        elif strategy_name == "CodeSIMA":
            return CodeSIMA
        elif strategy_name == "CodeSIMC":
            return CodeSIMC
```

### 3. ë°ì´í„°ì…‹ íŒ©í† ë¦¬ íŒ¨í„´
```python
class DatasetFactory:
    @staticmethod
    def create_dataset(dataset_name, **kwargs):
        dataset_class = DatasetFactory.get_dataset_class(dataset_name)
        
        if dataset_name.lower() in ["livecodebench", "lcb"] or dataset_name.startswith("lcb_"):
            if dataset_name.startswith("lcb_"):
                version = dataset_name.replace("lcb_", "")
            else:
                version = kwargs.get('release_version', 'release_v6')
            return dataset_class(release_version=version)
        else:
            return dataset_class(**kwargs)
```

## ğŸ“ˆ ê²°ê³¼ ë¶„ì„ ë° í•´ì„

### 1. ì‹¤í–‰ ê²°ê³¼ êµ¬ì¡°
```
results/
â””â”€â”€ {DATASET}/
    â””â”€â”€ {STRATEGY}/
        â””â”€â”€ {MODEL_NAME}/
            â””â”€â”€ {LANGUAGE}-{TEMPERATURE}-{TOP_P}-{PASS_AT_K}/
                â”œâ”€â”€ Run-{run_no}/
                â”‚   â”œâ”€â”€ Results.jsonl          # ê¸°ë³¸ ê²°ê³¼
                â”‚   â”œâ”€â”€ Summary.txt            # ìš”ì•½ í†µê³„
                â”‚   â”œâ”€â”€ Log.txt                # ì‹¤í–‰ ë¡œê·¸
                â”‚   â”œâ”€â”€ Results-ET.jsonl       # ET í‰ê°€ ê²°ê³¼
                â”‚   â”œâ”€â”€ Summary-ET.txt         # ET ìš”ì•½
                â”‚   â”œâ”€â”€ Results-EP.jsonl       # EP í‰ê°€ ê²°ê³¼
                â”‚   â”œâ”€â”€ Summary-EP.txt         # EP ìš”ì•½
                â”‚   â”œâ”€â”€ Results-LCB.jsonl      # LiveCodeBench ê²°ê³¼
                â”‚   â”œâ”€â”€ Summary-LCB.txt        # LiveCodeBench ìš”ì•½
                â”‚   â””â”€â”€ Report-LCB.json        # LiveCodeBench ìƒì„¸ ë¦¬í¬íŠ¸
```

### 2. ì„±ëŠ¥ ì§€í‘œ í•´ì„
- **Pass@1**: ì²« ë²ˆì§¸ ì‹œë„ì—ì„œ í†µê³¼í•œ ë¬¸ì œ ë¹„ìœ¨
- **ET (Execution Time)**: ì½”ë“œ ì‹¤í–‰ ì‹œê°„ ë¶„ì„
- **EP (Execution Pass)**: ì‹¤í–‰ í†µê³¼ìœ¨ ë¶„ì„
- **LCB (LiveCodeBench)**: ê²½ìŸ í”„ë¡œê·¸ë˜ë° íŠ¹í™” í‰ê°€

## ğŸ”® í–¥í›„ ë°œì „ ë°©í–¥

### 1. ëª¨ë¸ í™•ì¥
- **o3-mini**: Ollama ê¸°ë°˜ ë¡œì»¬ ëª¨ë¸ ì—°ë™
- **GPT-4o**: ìµœì‹  OpenAI ëª¨ë¸ ì§€ì›
- **Claude 3.5 Sonnet**: Anthropic ìµœì‹  ëª¨ë¸ ì§€ì›

### 2. ì „ëµ í™•ì¥
- **CodeSIM+**: ê°•í™”í•™ìŠµ ê¸°ë°˜ ì—ì´ì „íŠ¸ í˜‘ë ¥ ìµœì í™”
- **CodeSIM-Multi**: ë‹¤ì¤‘ ì–¸ì–´ ë™ì‹œ ìƒì„± ì§€ì›
- **CodeSIM-Adaptive**: ë¬¸ì œ ìœ í˜•ë³„ ìë™ ì „ëµ ì„ íƒ

### 3. í‰ê°€ ì‹œìŠ¤í…œ í™•ì¥
- **Code Quality Metrics**: ì½”ë“œ í’ˆì§ˆ ì§€í‘œ ì¶”ê°€
- **Runtime Performance**: ì‹¤í–‰ ì‹œê°„ ì„±ëŠ¥ ë¶„ì„
- **Memory Usage**: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„

## ğŸ“š ì°¸ê³  ìë£Œ

- **ë…¼ë¬¸**: CODESIM: Multi-Agent Code Generation with Simulation-Driven Planning and Debugging (NAACL 2025 Findings)
- **ì½”ë“œë² ì´ìŠ¤**: [GitHub Repository](https://github.com/your-repo/codesim)
- **ë°ì´í„°ì…‹**: HumanEval, MBPP, APPS, CodeContests, LiveCodeBench
- **í‰ê°€ í”„ë ˆì„ì›Œí¬**: ExecEval, Pass@k metrics

---

**Note**: ì´ READMEëŠ” CODESIM í”„ë ˆì„ì›Œí¬ì˜ ì‹¤ì œ êµ¬í˜„ ì½”ë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìœ¼ë©°, ë…¼ë¬¸ì—ì„œ ì œì‹œí•œ ì´ë¡ ì  ê°œë…ë“¤ì´ ì–´ë–»ê²Œ ì‹¤ì œë¡œ êµ¬í˜„ë˜ì—ˆëŠ”ì§€ ìƒì„¸íˆ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤.
