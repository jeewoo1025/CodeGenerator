# CODESIM ë¦¬ë·° (NAACL 2025 Findings)

## ğŸ“‹ ë…¼ë¬¸ ì»¨í…ìŠ¤íŠ¸

**CODESIM**ì€ NAACL 2025 Findingsì—ì„œ ì œì‹œëœ í˜ì‹ ì ì¸ multi-agent code generation í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ê¸°ì¡´ MapCoderì˜ "multiple ungrounded exemplars" ì ‘ê·¼ë²•ì„ ê°œì„ í•˜ì—¬ **"single exemplar" ê¸°ë°˜ì˜ simulation-driven planning and debugging**ì„ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤.

### ğŸš€ í•µì‹¬ í˜ì‹ 
- **3-Agent Architecture**: Planning Agent, Coding Agent, Debugging Agentì˜ í˜‘ë ¥ì  êµ¬ì¡°
- **Simulation-Driven Verification**: Step-by-step ì‹œë®¬ë ˆì´ì…˜ì„ í†µí•œ ê³„íš ê²€ì¦
- **Internal Debugging**: ì™¸ë¶€ ë„êµ¬ ì—†ì´ ì‹œë®¬ë ˆì´ì…˜ ê¸°ë°˜ ë‚´ë¶€ ë””ë²„ê¹…
- **Human-like Perception**: ì¸ê°„ì˜ ì•Œê³ ë¦¬ì¦˜ ì‹œê°ì  ê²€ì¦ ë°©ì‹ êµ¬í˜„

### ğŸ“Š SOTA ì„±ëŠ¥ ë‹¬ì„±
- **HumanEval**: 95.1% (Pass@1)
- **MBPP**: 90.7% (Pass@1)  
- **APPS**: 22% (Pass@1)
- **CodeContests**: 29.1% (Pass@1)

## ğŸ—ï¸ ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Planning Agent â”‚â”€â”€â”€â–¶â”‚  Coding Agent   â”‚â”€â”€â”€â–¶â”‚ Debugging Agent â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ Problem       â”‚    â”‚ â€¢ Plan â†’ Code   â”‚    â”‚ â€¢ Internal      â”‚
â”‚   Understanding â”‚    â”‚ â€¢ Execution     â”‚    â”‚   Simulation    â”‚
â”‚ â€¢ Exemplar      â”‚    â”‚ â€¢ Generation    â”‚    â”‚ â€¢ Bug Detection â”‚
â”‚   Recall        â”‚    â”‚                 â”‚    â”‚ â€¢ Code Fix      â”‚
â”‚ â€¢ Algorithm     â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚   Design        â”‚    â”‚                 â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Plan Simulation â”‚    â”‚ Code Execution  â”‚    â”‚ Test Validation â”‚
â”‚ & Verification  â”‚    â”‚ & Evaluation    â”‚    â”‚ & Refinement    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ” í•µì‹¬ êµ¬í˜„ ë¶„ì„

### 1. 3-Agent êµ¬ì¡° êµ¬í˜„

#### Planning Agent (`CodeSIM.py` lines 200-280)
```python
# í•µì‹¬ ê³„íš ìƒì„± ë¡œì§
input_for_planning = [
    {
        "role": "user", 
        "content": prompt_for_planning.format(
            problem=problem,
            language=self.language,
        )
    },
]

# ê³„íš êµ¬ì¡°í™”
if "### Plan" not in response:
    plan = f"### Plan\n\n{response}"
else:
    plan = response[response.rfind("### Plan"):]
```

**êµ¬í˜„ íŠ¹ì§•:**
- **Problem Understanding**: ë¬¸ì œ ìœ í˜• ë° ì œì•½ì‚¬í•­ ë¶„ì„
- **Exemplar Recall**: ê´€ë ¨ ì˜ˆì œ ë¬¸ì œ íšŒìƒ ë° ì•Œê³ ë¦¬ì¦˜ ë¶„ì„
- **Algorithm Design**: ìµœì  ì•Œê³ ë¦¬ì¦˜ ì„ íƒ ë° íŠœí† ë¦¬ì–¼ ì œê³µ
- **Step-by-step Planning**: ìƒì„¸í•œ ì‹¤í–‰ ê³„íš ìˆ˜ë¦½

#### Coding Agent (`CodeSIM.py` lines 320-350)
```python
# ê³„íš ê¸°ë°˜ ì½”ë“œ ìƒì„±
input_for_final_code_generation = [
    {
        "role": "user",
        "content": prompt_for_code_generation.format(
            problem_with_planning=problem_with_planning,
            language=self.language,
            std_input_prompt=std_input_prompt,
        )
    }
]

code = parse_response(response)
```

**êµ¬í˜„ íŠ¹ì§•:**
- **Plan-to-Code Translation**: ê²€ì¦ëœ ê³„íšì„ ì‹¤í–‰ ê°€ëŠ¥í•œ ì½”ë“œë¡œ ë³€í™˜
- **Language-Specific Generation**: í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë³„ ìµœì í™”ëœ ì½”ë“œ ìƒì„±
- **Standard I/O Handling**: ê²½ìŸ í”„ë¡œê·¸ë˜ë° í™˜ê²½ì— ìµœì í™”ëœ ì…ì¶œë ¥ ì²˜ë¦¬

#### Debugging Agent (`CodeSIM.py` lines 360-420)
```python
# ë‚´ë¶€ ë””ë²„ê¹… ë©”ì»¤ë‹ˆì¦˜
for debug_no in range(1, self.max_debug_try + 1):
    input_for_debugging = [
        {
            "role": "user",
            "content": prompt_for_debugging.format(
                problem_with_planning=problem_with_planning,
                code=code,
                language=self.language,
                test_log=test_log,
                std_input_prompt=std_input_prompt,
            )
        }
    ]
    
    code = parse_response(response)
    passed, test_log = self.check(data_row, additional_io, code)
    
    if passed:
        break
```

**êµ¬í˜„ íŠ¹ì§•:**
- **Internal Simulation**: ì™¸ë¶€ ë„êµ¬ ì—†ì´ ì‹œë®¬ë ˆì´ì…˜ ê¸°ë°˜ ë²„ê·¸ íƒì§€
- **Step-by-step Analysis**: ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ì˜ ë‹¨ê³„ë³„ ë¶„ì„
- **Plan-Code Alignment**: ê³„íšê³¼ ì½”ë“œ ê°„ ë¶ˆì¼ì¹˜ ì  ê²€ì¶œ
- **Iterative Refinement**: ìµœëŒ€ 5íšŒê¹Œì§€ ë°˜ë³µì  ì½”ë“œ ê°œì„ 

### 2. Simulation-Driven ì ‘ê·¼ë²• êµ¬í˜„

#### Plan Verification (`CodeSIM.py` lines 290-320)
```python
# ê³„íš ì‹œë®¬ë ˆì´ì…˜ ë° ê²€ì¦
input_for_simulation = [
    {
        "role": "user",
        "content": prompt_for_simulation.format(
            problem_with_planning=problem_with_planning,
            language=self.language,
        )
    },
]

# ê³„íš ìˆ˜ì • í•„ìš”ì„± íŒë‹¨
if "Plan Modification Needed" in response and \
    "No Plan Modification Needed" not in response:
    
    # ê³„íš ì •ì œ ë‹¨ê³„
    input_for_plan_refinement = [
        {
            "role": "user",
            "content": prompt_for_plan_refinement.format(
                problem_with_planning=problem_with_planning,
                language=self.language,
                critique=response
            )
        },
    ]
```

**êµ¬í˜„ íŠ¹ì§•:**
- **Manual Simulation**: ì½”ë“œ ì—†ì´ ìˆ˜ë™ìœ¼ë¡œ ê³„íš ë‹¨ê³„ë³„ ì‹¤í–‰
- **Output Comparison**: ì˜ˆìƒ ì¶œë ¥ê³¼ ì‹¤ì œ ì¶œë ¥ ë¹„êµ ê²€ì¦
- **Plan Critique**: ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ ê¸°ë°˜ ê³„íš ë¹„íŒì  ë¶„ì„
- **Iterative Refinement**: ìµœëŒ€ 5íšŒê¹Œì§€ ê³„íš ê°œì„  ë°˜ë³µ

#### Internal Debugging Simulation
```python
# ë””ë²„ê¹… í”„ë¡¬í”„íŠ¸ì˜ ì‹œë®¬ë ˆì´ì…˜ ì§€ì‹œì‚¬í•­
prompt_for_debugging = """
### Simulation with failed test case
To detect where is the bug follow following steps:
    - Take a sample test case where it fails.
    - Take the input go through each step according to the plan
    - You will get a output that must be different from the expected output.

### Debugging Notes
- Based on this simulation detect any of the following cases:
    - Plan is wrong
    - Plan is correct but plan to code generation is wrong.
- Finally, discuss how to correct this code.
"""
```

**êµ¬í˜„ íŠ¹ì§•:**
- **Step-by-step Execution**: ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ì˜ ë‹¨ê³„ë³„ ì‹¤í–‰ ì‹œë®¬ë ˆì´ì…˜
- **Plan-Code Mismatch Detection**: ê³„íšê³¼ ì½”ë“œ ê°„ ë¶ˆì¼ì¹˜ ì  ì‹ë³„
- **Root Cause Analysis**: ë²„ê·¸ì˜ ê·¼ë³¸ ì›ì¸ ë¶„ì„
- **Corrective Action Planning**: ìˆ˜ì • ë°©ì•ˆ ìˆ˜ë¦½

### 3. Multi-Agent ê°„ í†µì‹  ë° ë°ì´í„° íë¦„

#### Agent ê°„ ë°ì´í„° ì „ë‹¬ êµ¬ì¡°
```python
# Planning â†’ Coding â†’ Debugging ë°ì´í„° íë¦„
problem_with_planning = f"## Problem:\n{problem}\n\n{plan}"

# ê° ë‹¨ê³„ì—ì„œ ì´ì „ ë‹¨ê³„ì˜ ê²°ê³¼ë¥¼ ì…ë ¥ìœ¼ë¡œ í™œìš©
input_for_final_code_generation = [
    {
        "role": "user",
        "content": prompt_for_code_generation.format(
            problem_with_planning=problem_with_planning,  # ê³„íš í¬í•¨
            language=self.language,
            std_input_prompt=std_input_prompt,
        )
    }
]

input_for_debugging = [
    {
        "role": "user", 
        "content": prompt_for_debugging.format(
            problem_with_planning=problem_with_planning,  # ê³„íš + ì½”ë“œ
            code=code,
            language=self.language,
            test_log=test_log,
            std_input_prompt=std_input_prompt,
        )
    }
]
```

#### ë°˜ë³µì  ê°œì„  ë©”ì»¤ë‹ˆì¦˜
```python
# Planning ë°˜ë³µ (ìµœëŒ€ 5íšŒ)
for plan_no in range(1, self.max_plan_try + 1):
    # ... planning logic ...
    if passed:
        break

# Debugging ë°˜ë³µ (ìµœëŒ€ 5íšŒ)  
for debug_no in range(1, self.max_debug_try + 1):
    # ... debugging logic ...
    if passed:
        break
```

## ğŸ§ª ë²¤ì¹˜ë§ˆí¬ ë° í‰ê°€ ì‹œìŠ¤í…œ

### ExecEval ì—°ë™ êµ¬í˜„
```python
# ì½”ë“œ ì‹¤í–‰ ë° í‰ê°€
def check(self, data_row: dict, additional_io: List[str], code: str) -> bool:
    passed_sample, test_log_sample = self.data.evaluate_sample_io(
        data_row, code, self.language
    )
    
    passed_additional, test_log_additional = self.data.evaluate_additional_io(
        data_row[self.data.id_key], additional_io, code, self.language
    )
    
    return passed_sample & passed_additional, test_log
```

### Pass@1 í‰ê°€ ë©”íŠ¸ë¦­
```python
# ê²°ê³¼ ì§‘ê³„ ë° ìš”ì•½
gen_summary(RESULTS_PATH, SUMMARY_PATH)

# ET/EP ë°ì´í„°ì…‹ ìƒì„±
if "human" in DATASET.lower():
    generate_et_dataset_human(RESULTS_PATH, ET_RESULTS_PATH)
    gen_summary(ET_RESULTS_PATH, ET_SUMMARY_PATH)
```

## ğŸ”§ ì‹¤í–‰ ê°€ì´ë“œ

### ê¸°ë³¸ ì‹¤í–‰ ëª…ë ¹ì–´
```bash
# HumanEval ë°ì´í„°ì…‹ìœ¼ë¡œ CodeSIM ì‹¤í–‰
python src/main.py --dataset HumanEval --strategy CodeSIM --model ChatGPT

# MBPP ë°ì´í„°ì…‹ìœ¼ë¡œ CodeSIM ì‹¤í–‰  
python src/main.py --dataset MBPP --strategy CodeSIM --model ChatGPT

# APPS ë°ì´í„°ì…‹ìœ¼ë¡œ CodeSIM ì‹¤í–‰
python src/main.py --dataset APPS --strategy CodeSIM --model ChatGPT

# LiveCodeBench ë°ì´í„°ì…‹ìœ¼ë¡œ CodeSIM ì‹¤í–‰
python src/main.py --dataset LiveCodeBench --strategy CodeSIM --model ChatGPT
```

## ğŸ”„ Dataset ì‹¤í–‰ ë‚´ë¶€ í”„ë¡œì„¸ìŠ¤

### 1. ë°ì´í„°ì…‹ ë¡œë”© ë° ì´ˆê¸°í™” ê³¼ì •

#### Dataset Factory íŒ¨í„´ì„ í†µí•œ ë™ì  ìƒì„±
```python
# src/main.pyì—ì„œ ë°ì´í„°ì…‹ ìƒì„±
if DATASET.lower() in ["livecodebench", "lcb"] or DATASET.startswith("lcb_"):
    # LiveCodeBench íŠ¹ë³„ ì²˜ë¦¬
    version = args.lcb_version
    strategy = PromptingFactory.get_prompting_class(STRATEGY)(
        model=ModelFactory.get_model_class(MODEL_PROVIDER_NAME)(...),
        data=DatasetFactory.create_dataset(DATASET, release_version=version),
        language=LANGUAGE,
        pass_at_k=PASS_AT_K,
        results=Results(RESULTS_PATH),
        verbose=VERBOSE
    )
else:
    # ì¼ë°˜ ë°ì´í„°ì…‹ ì²˜ë¦¬
    strategy = PromptingFactory.get_prompting_class(STRATEGY)(
        model=ModelFactory.get_model_class(MODEL_PROVIDER_NAME)(...),
        data=DatasetFactory.create_dataset(DATASET),
        language=LANGUAGE,
        pass_at_k=PASS_AT_K,
        results=Results(RESULTS_PATH),
        verbose=VERBOSE
    )
```

#### ë°ì´í„°ì…‹ë³„ íŠ¹í™” ì²˜ë¦¬
```python
# src/datasets/DatasetFactory.py
class DatasetFactory:
    @staticmethod
    def create_dataset(dataset_name, **kwargs):
        dataset_class = DatasetFactory.get_dataset_class(dataset_name)
        
        # LiveCodeBench: ë²„ì „ë³„ ë¦´ë¦¬ì¦ˆ ì§€ì›
        if dataset_name.lower() in ["livecodebench", "lcb"] or dataset_name.startswith("lcb_"):
            if dataset_name.startswith("lcb_"):
                version = dataset_name.replace("lcb_", "")
            else:
                version = kwargs.get('release_version', 'release_v6')
            return dataset_class(release_version=version)
        else:
            return dataset_class(**kwargs)
```

### 2. ë°ì´í„°ì…‹ ì‹¤í–‰ ì›Œí¬í”Œë¡œìš°

#### Step 1: ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬
```python
# src/datasets/Dataset.py - ê¸°ë³¸ ë°ì´í„°ì…‹ í´ë˜ìŠ¤
class Dataset(object):
    def __init__(self, path: str):
        self.path = path
        self.data = None
        self.id_key = ""
        self.load()  # JSONL íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ
    
    def load(self):
        self.data = read_jsonl(self.path)  # JSONL í˜•ì‹ ë°ì´í„° íŒŒì‹±
    
    def __len__(self):
        return len(self.data)  # ë°ì´í„°ì…‹ í¬ê¸° ë°˜í™˜
    
    def __getitem__(self, idx):
        return self.data[idx]  # ì¸ë±ìŠ¤ ê¸°ë°˜ ë°ì´í„° ì ‘ê·¼
```

#### Step 2: ë¬¸ì œë³„ í”„ë¡¬í”„íŠ¸ ìƒì„±
```python
# src/datasets/HumanEvalDataset.py - HumanEval íŠ¹í™” ì²˜ë¦¬
class HumanDataset(Dataset):
    def __init__(self, path: str = HUMAN_DATA_PATH):
        super().__init__(path)
        self.id_key = "task_id"  # ê³ ìœ  ì‹ë³„ì í‚¤ ì„¤ì •
    
    @staticmethod
    def get_prompt(item):
        # í”„ë¡¬í”„íŠ¸ ë˜ëŠ” í…ìŠ¤íŠ¸ í•„ë“œì—ì„œ ë¬¸ì œ ì„¤ëª… ì¶”ì¶œ
        if "prompt" in item:
            return f"{item['prompt'].strip()}"
        elif "text" in item:
            return f"{item['text'].strip()}"
        else:
            raise Exception("No prompt or text in item")
```

#### Step 3: ì½”ë“œ ì‹¤í–‰ ë° í‰ê°€
```python
# src/datasets/HumanEvalDataset.py - í‰ê°€ ë¡œì§
def evaluate_sample_io(self, item: dict, cur_imp: str, language: str):
    # ìƒ˜í”Œ I/O í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    return evaluate_io(
        sample_io=item["sample_io"],  # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
        completion=cur_imp,           # ìƒì„±ëœ ì½”ë“œ
    )

def evaluate_additional_io(self, id: int, io: List[str], cur_imp: str, language: str):
    # ì¶”ê°€ I/O í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    if len(io) == 0:
        return True, ""
    
    return evaluate_io(
        sample_io=io,      # ì¶”ê°€ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
        completion=cur_imp, # ìƒì„±ëœ ì½”ë“œ
    )
```

### 3. ì‹¤í–‰ ê²°ê³¼ ì €ì¥ ë° ë¶„ì„

#### ê²°ê³¼ íŒŒì¼ êµ¬ì¡°
```
results/
â””â”€â”€ {DATASET}/                    # ë°ì´í„°ì…‹ë³„ ë¶„ë¥˜
    â””â”€â”€ {STRATEGY}/              # ì „ëµë³„ ë¶„ë¥˜
        â””â”€â”€ {MODEL_NAME}/        # ëª¨ë¸ë³„ ë¶„ë¥˜
            â””â”€â”€ {LANGUAGE}-{TEMPERATURE}-{TOP_P}-{PASS_AT_K}/
                â”œâ”€â”€ Run-{run_no}/ # ì‹¤í–‰ ë²ˆí˜¸ë³„ ë¶„ë¥˜
                â”‚   â”œâ”€â”€ Results.jsonl          # ê¸°ë³¸ ì‹¤í–‰ ê²°ê³¼
                â”‚   â”œâ”€â”€ Summary.txt            # í†µê³„ ìš”ì•½
                â”‚   â”œâ”€â”€ Log.txt                # ìƒì„¸ ì‹¤í–‰ ë¡œê·¸
                â”‚   â”œâ”€â”€ Results-ET.jsonl       # Execution Time ê²°ê³¼
                â”‚   â”œâ”€â”€ Results-EP.jsonl       # Execution Pass ê²°ê³¼
                â”‚   â””â”€â”€ Results-LCB.jsonl      # LiveCodeBench íŠ¹í™” ê²°ê³¼
```

#### ì‹¤í–‰ ë¡œê·¸ ë° ëª¨ë‹ˆí„°ë§
```python
# src/main.py - ì‹¤í–‰ ë¡œê·¸ ê´€ë¦¬
if STORE_LOG_IN_FILE.lower() == 'yes':
    sys.stdout = open(LOGS_PATH, mode="a", encoding="utf-8")

# ì‹¤í–‰ ì‹œì‘/ì¢…ë£Œ ë¡œê·¸
if CONTINUE == "no" and VERBOSE >= VERBOSE_MINIMAL:
    print(f"""
##################################################
Experiment start {RUN_NAME}, Time: {datetime.now()}
###################################################
""")

# ê²°ê³¼ ìš”ì•½ ìƒì„±
gen_summary(RESULTS_PATH, SUMMARY_PATH)
```

## ğŸ¯ ë‹¤ë¥¸ Prompting ì „ëµë“¤ì˜ êµ¬í˜„ ë°©ì‹

### 1. Chain-of-Thought (CoT) ì „ëµ

#### í•µì‹¬ ì•„ì´ë””ì–´
CoTëŠ” **"Let's think step by step"** ì ‘ê·¼ë²•ìœ¼ë¡œ, ë¬¸ì œë¥¼ ë‹¨ê³„ë³„ë¡œ ë¶„ì„í•˜ì—¬ í•´ê²°í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.

#### êµ¬í˜„ êµ¬ì¡° (`src/promptings/CoT.py`)
```python
class CoTStrategy(BaseStrategy):
    def run_single_pass(self, data_row: dict):
        # HumanEval ë°ì´í„°ì…‹ ì „ìš© í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        if type(self.data) == HumanDataset:
            planning_prompt = """
def encrypt(s):
    '''
    Create a function encrypt that takes a string as an argument and
    returns a string encrypted with the alphabet being rotated. 
    The alphabet should be rotated in a manner such that the letters 
    shift down by two multiplied to two places.
    For example:
    encrypt('hi') returns 'lm'
    encrypt('asdfghjkl') returns 'ewhjklnop'
    encrypt('gf') returns 'kj'
    encrypt('et') returns 'ix'
    '''
    # Let's think step by step.

    # Define the alphabet as a string
    d = 'abcdefghijklmnopqrstuvwxyz'
    
    # Initialize an empty string to store the encrypted result
    out = ''
    
    # Iterate through each character in the input string
    for c in s:
        # Check if the character is a letter in the alphabet
        if c in c:
            # Find the index of the current letter in the alphabet
            index = d.index(c)
            
            # Rotate the alphabet by two multiplied to two places
            # Use modulo 26 to handle wrapping around the alphabet
            rotated_index = (index + 2 * 2) % 26
            
            # Append the encrypted letter to the result string
            out += d[rotated_index]
        else:
            # If the character is not a letter, append it unchanged
            out += c
    
    # Return the final encrypted string
    return out
    """
```

**CoTì˜ íŠ¹ì§•:**
- **Step-by-step Reasoning**: ê° ë‹¨ê³„ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì„¤ëª…
- **Exemplar-based Learning**: ì˜ˆì œ ë¬¸ì œì™€ í•´ê²° ê³¼ì •ì„ í¬í•¨
- **Direct Code Generation**: ì‚¬ê³  ê³¼ì •ê³¼ í•¨ê»˜ ì½”ë“œë¥¼ ì§ì ‘ ìƒì„±
- **No Iteration**: ë‹¨ì¼ íŒ¨ìŠ¤ë¡œ í•´ê²° (ë°˜ë³µ ì—†ìŒ)

### 2. MapCoder ì „ëµ

#### í•µì‹¬ ì•„ì´ë””ì–´
MapCoderëŠ” **"multiple ungrounded exemplars"**ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ, ì—¬ëŸ¬ ì˜ˆì œë¥¼ ì°¸ê³ í•˜ì—¬ ë§¤í•‘ ê¸°ë°˜ìœ¼ë¡œ ì½”ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

#### êµ¬í˜„ êµ¬ì¡° (`src/promptings/MapCoder.py`)
```python
class MapCoder(BaseStrategy):
    def __init__(self, k: int = 3, t: int = 5, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.k = k  # exemplar ê°œìˆ˜
        self.t = t  # ì‹œë„ íšŸìˆ˜

    def xml_to_dict(self, element):
        # XML ì‘ë‹µì„ ë”•ì…”ë„ˆë¦¬ë¡œ íŒŒì‹±
        result = {}
        for child in element:
            if child:
                child_data = self.xml_to_dict(child)
                if child.tag in result:
                    if isinstance(result[child.tag], list):
                        result[child.tag].append(child_data)
                    else:
                        result[child.tag] = [result[child.tag], child_data]
                else:
                    result[child.tag] = child_data
            else:
                result[child.tag] = child.text
        return result

    def parse_xml(self, response: str) -> dict:
        # XML ì‘ë‹µ íŒŒì‹± ë° êµ¬ì¡°í™”
        if '```xml' in response:
            response = response.replace('```xml', '')
        if '```' in response:
            response = response.replace('```', '')

        try:
            root = ET.fromstring(response)
        except:
            try:
                root = ET.fromstring('<root>\n' + response + '\n</root>')
            except:
                root = ET.fromstring('<root>\n' + response)
        return self.xml_to_dict(root)
```

**MapCoderì˜ íŠ¹ì§•:**
- **Multiple Exemplars**: kê°œì˜ ì˜ˆì œë¥¼ ë™ì‹œì— ì°¸ê³ 
- **XML-based Parsing**: êµ¬ì¡°í™”ëœ ì‘ë‹µì„ XMLë¡œ íŒŒì‹±
- **Iterative Refinement**: të²ˆì˜ ì‹œë„ë¥¼ í†µí•œ ì ì§„ì  ê°œì„ 
- **No Simulation**: ê³„íš ê²€ì¦ ì—†ì´ ì§ì ‘ ì½”ë“œ ìƒì„±

### 3. Self-Planning ì „ëµ

#### í•µì‹¬ ì•„ì´ë””ì–´
Self-Planningì€ **"ìê¸° ê³„íš ìˆ˜ë¦½"**ì„ í†µí•´ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ, LLMì´ ìŠ¤ìŠ¤ë¡œ ê³„íšì„ ì„¸ìš°ê³  ì‹¤í–‰í•©ë‹ˆë‹¤.

#### êµ¬í˜„ êµ¬ì¡° (`src/promptings/SelfPlanning.py`)
```python
class SelfPlanningStrategy(BaseStrategy):
    def run_single_pass(self, data_row: dict):
        # HumanEval ë°ì´í„°ì…‹ ì „ìš© ê³„íš í”„ë¡¬í”„íŠ¸
        if type(self.data) == HumanDataset:
            planning_prompt = """
def encrypt(s):
    '''
    Create a function encrypt that takes a string as an argument and returns a string encrypted with the alphabet being rotated. The alphabet should be rotated in a manner such that the letters shift down by two multiplied to two places.
    For example:
    encrypt('hi') returns 'lm'
    encrypt('asdfghjkl') returns 'ewhjklnop'
    encrypt('gf') returns 'kj'
    encrypt('et') returns 'ix'
    Let's think step by step.
    1. Create a alphabet, bias two places multiplied by two.
    2. Loop the input, find the latter bias letter in alphabet.
    3. Return result.
    ''' 

def check_if_last_char_is_a_letter(txt):
    ''' 
    Create a function that returns True if the last character of a given string is an alphabetical character and is not a part of a word, and False otherwise. Note: 'word' is a group of characters separated by space.
    Examples:
    check_if_last_char_is_a_letter('apple pie') â†’ False
    check_if_last_char_is_a_letter('apple pi e') â†’ True
    check_if_last_char_is_a_letter('apple pi e ') â†’ False
    check_if_last_char_is_a_letter('') â†’ False
    Let's think step by step.
    1. If the string is empty, return False.
    2. If the string does not end with a alphabetical character, return False.
    3. Split the given string into a list of words.
    4. Check if the length of the last word is equal to 1.
    '''
    """
```

**Self-Planningì˜ íŠ¹ì§•:**
- **Self-Generated Plans**: LLMì´ ìŠ¤ìŠ¤ë¡œ ê³„íšì„ ìˆ˜ë¦½
- **Step-by-step Instructions**: ëª…í™•í•œ ë‹¨ê³„ë³„ ì§€ì‹œì‚¬í•­
- **Exemplar Integration**: ì˜ˆì œì™€ ê³„íšì„ í†µí•©í•˜ì—¬ ì œê³µ
- **No External Validation**: ì™¸ë¶€ ê²€ì¦ ì—†ì´ ìì²´ ê³„íš ì‹¤í–‰

### 4. Direct ì „ëµ

#### í•µì‹¬ ì•„ì´ë””ì–´
DirectëŠ” **"ì§ì ‘ì ì¸ ì½”ë“œ ìƒì„±"** ë°©ì‹ìœ¼ë¡œ, ë³µì¡í•œ í”„ë¡¬í”„íŠ¸ ì—†ì´ ë¬¸ì œ ì„¤ëª…ë§Œìœ¼ë¡œ ì½”ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

#### êµ¬í˜„ êµ¬ì¡° (`src/promptings/Direct.py`)
```python
class DirectStrategy(BaseStrategy):
    def run_single_pass(self, data_row: dict):
        # ê°€ì¥ ë‹¨ìˆœí•œ ë°©ì‹: ë¬¸ì œ ì„¤ëª…ë§Œìœ¼ë¡œ ì½”ë“œ ìƒì„±
        prompt = self.data.get_prompt(data_row)
        
        # LLMì— ì§ì ‘ ì „ë‹¬í•˜ì—¬ ì½”ë“œ ìƒì„±
        response = self.gpt_chat([
            {
                "role": "user",
                "content": prompt
            }
        ])
        
        # ì‘ë‹µì—ì„œ ì½”ë“œ ì¶”ì¶œ
        code = parse_response(response)
        return code
```

**Directì˜ íŠ¹ì§•:**
- **Minimal Prompting**: ìµœì†Œí•œì˜ í”„ë¡¬í”„íŠ¸ë§Œ ì‚¬ìš©
- **No Planning**: ê³„íš ìˆ˜ë¦½ ê³¼ì • ì—†ìŒ
- **No Exemplars**: ì˜ˆì œ ì°¸ê³  ì—†ìŒ
- **Fastest Execution**: ê°€ì¥ ë¹ ë¥¸ ì‹¤í–‰ ì†ë„

### 5. ì „ëµë³„ ì„±ëŠ¥ ë¹„êµ ë° ì„ íƒ ê°€ì´ë“œ

#### ë³µì¡ë„ vs ì„±ëŠ¥ íŠ¸ë ˆì´ë“œì˜¤í”„
```
ë³µì¡ë„: Direct < CoT < SelfPlanning < MapCoder < CodeSIM
ì„±ëŠ¥:   Direct < CoT < SelfPlanning < MapCoder < CodeSIM
ì†ë„:   Direct > CoT > SelfPlanning > MapCoder > CodeSIM
```

#### ë°ì´í„°ì…‹ë³„ ê¶Œì¥ ì „ëµ
- **HumanEval/MBPP**: CodeSIM (ë†’ì€ ì •í™•ë„ ìš”êµ¬)
- **APPS/CodeContests**: CodeSIM ë˜ëŠ” MapCoder (ë³µì¡í•œ ë¬¸ì œ)
- **LiveCodeBench**: CodeSIM (ê²½ìŸ í”„ë¡œê·¸ë˜ë° ìµœì í™”)
- **ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘**: Direct ë˜ëŠ” CoT
- **ê· í˜•ì¡íŒ ì ‘ê·¼**: SelfPlanning

#### ì „ëµ ì„ íƒ ê¸°ì¤€
```python
# src/main.pyì—ì„œ ì „ëµ ì„ íƒ
STRATEGY = args.strategy  # ì‚¬ìš©ìê°€ ì„ íƒí•œ ì „ëµ

# ì „ëµë³„ íŠ¹ì„±ì— ë”°ë¥¸ ìë™ ìµœì í™”
if STRATEGY == "CodeSIM":
    # ê³„íš ê²€ì¦ ë° ë””ë²„ê¹… í™œì„±í™”
    max_plan_try = 5
    max_debug_try = 5
elif STRATEGY == "MapCoder":
    # exemplar ê¸°ë°˜ ì ‘ê·¼
    k = 3  # exemplar ê°œìˆ˜
    t = 5  # ì‹œë„ íšŸìˆ˜
elif STRATEGY == "Direct":
    # ë‹¨ìˆœí•œ ì§ì ‘ ìƒì„±
    # ì¶”ê°€ ì˜µì…˜ ì—†ìŒ
```

### ê³ ê¸‰ ì‹¤í–‰ ì˜µì…˜
```bash
# ê³„íš ì‹œë„ íšŸìˆ˜ ë° ë””ë²„ê¹… ì‹œë„ íšŸìˆ˜ ì¡°ì •
python src/main.py \
    --dataset HumanEval \
    --strategy CodeSIM \
    --model ChatGPT \
    --temperature 0 \
    --top_p 0.95 \
    --pass_at_k 1
```

### ëª¨ë¸ë³„ ì‹¤í–‰
```bash
# Gemini ëª¨ë¸ ì‚¬ìš©
python src/main.py --dataset HumanEval --strategy CodeSIM --model gemini-pro --model_provider Gemini

# Groq ëª¨ë¸ ì‚¬ìš©
python src/main.py --dataset HumanEval --strategy CodeSIM --model mixtral-8x7b-32768 --model_provider Groq

# Anthropic ëª¨ë¸ ì‚¬ìš©
python src/main.py --dataset HumanEval --strategy CodeSIM --model claude-3-sonnet-20240229 --model_provider Anthropic
```

## ğŸ“Š ì½”ë“œ êµ¬ì¡° ìƒì„¸ ë¶„ì„

### í•µì‹¬ í´ë˜ìŠ¤ êµ¬ì¡°
```
src/
â”œâ”€â”€ promptings/
â”‚   â”œâ”€â”€ CodeSIM.py              # ë©”ì¸ CodeSIM êµ¬í˜„
â”‚   â”œâ”€â”€ variations/
â”‚   â”‚   â”œâ”€â”€ CodeSIMWPVD.py     # With Plan Verification & Debugging
â”‚   â”‚   â”œâ”€â”€ CodeSIMWD.py        # With Debugging
â”‚   â”‚   â”œâ”€â”€ CodeSIMWPV.py       # With Plan Verification
â”‚   â”‚   â”œâ”€â”€ CodeSIMA.py         # Analogical variation
â”‚   â”‚   â””â”€â”€ CodeSIMC.py         # Competitive programming
â”‚   â”œâ”€â”€ Base.py                 # ê¸°ë³¸ ì „ëµ í´ë˜ìŠ¤
â”‚   â””â”€â”€ PromptingFactory.py     # ì „ëµ íŒ©í† ë¦¬
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ ModelFactory.py         # ëª¨ë¸ íŒ©í† ë¦¬
â”‚   â”œâ”€â”€ OpenAI.py               # OpenAI ëª¨ë¸ êµ¬í˜„
â”‚   â”œâ”€â”€ Gemini.py               # Gemini ëª¨ë¸ êµ¬í˜„
â”‚   â””â”€â”€ Anthropic.py            # Anthropic ëª¨ë¸ êµ¬í˜„
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ DatasetFactory.py       # ë°ì´í„°ì…‹ íŒ©í† ë¦¬
â”‚   â”œâ”€â”€ HumanEvalDataset.py     # HumanEval ë°ì´í„°ì…‹
â”‚   â”œâ”€â”€ MBPPDataset.py          # MBPP ë°ì´í„°ì…‹
â”‚   â”œâ”€â”€ APPSDataset.py          # APPS ë°ì´í„°ì…‹
â”‚   â””â”€â”€ LiveCodeBenchDataset.py # LiveCodeBench ë°ì´í„°ì…‹
â””â”€â”€ evaluations/
    â”œâ”€â”€ func_evaluate.py         # í•¨ìˆ˜ í‰ê°€ ì—”ì§„
    â””â”€â”€ executor_utils.py        # ì‹¤í–‰ ìœ í‹¸ë¦¬í‹°
```

### ì£¼ìš” í•¨ìˆ˜ ë¶„ì„

#### 1. `run_single_pass()` - ë©”ì¸ ì‹¤í–‰ ë¡œì§
```python
def run_single_pass(self, data_row: dict):
    # 1. ë¬¸ì œ ë¶„ì„ ë° ì¶”ê°€ I/O ìˆ˜ì§‘
    problem = self.data.get_prompt(data_row)
    additional_io = []
    
    # 2. Planning Phase (ìµœëŒ€ 5íšŒ)
    for plan_no in range(1, self.max_plan_try + 1):
        # ê³„íš ìƒì„± â†’ ì‹œë®¬ë ˆì´ì…˜ ê²€ì¦ â†’ ê³„íš ì •ì œ
        # ì½”ë“œ ìƒì„± â†’ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        
        # 3. Debugging Phase (ìµœëŒ€ 5íšŒ)
        for debug_no in range(1, self.max_debug_try + 1):
            # ë‚´ë¶€ ì‹œë®¬ë ˆì´ì…˜ â†’ ë²„ê·¸ íƒì§€ â†’ ì½”ë“œ ìˆ˜ì •
            # í…ŒìŠ¤íŠ¸ ì¬ì‹¤í–‰
            
        if passed:
            break
```

#### 2. `check()` - ì½”ë“œ ê²€ì¦ ë¡œì§
```python
def check(self, data_row: dict, additional_io: List[str], code: str) -> bool:
    # ìƒ˜í”Œ I/O í‰ê°€
    passed_sample, test_log_sample = self.data.evaluate_sample_io(
        data_row, code, self.language
    )
    
    # ì¶”ê°€ I/O í‰ê°€  
    passed_additional, test_log_additional = self.data.evaluate_additional_io(
        data_row[self.data.id_key], additional_io, code, self.language
    )
    
    # í†µí•© ê²°ê³¼ ë°˜í™˜
    return passed_sample & passed_additional, test_log
```

## ğŸš€ ì„±ëŠ¥ ìµœì í™” êµ¬í˜„

### 1. ê²½ìŸ í”„ë¡œê·¸ë˜ë° ìµœì í™”
```python
# APPS, CodeContest, XCode ë°ì´í„°ì…‹ ìµœì í™”
self.is_competitive = type(self.data) == APPSDataset or \
    type(self.data) == CodeContestDataset or \
    type(self.data) == XCodeDataset

if self.is_competitive:
    std_input_prompt = """
    - Strictly follow the sample input and output format. 
    - The input should be taken from Standard input and output should be given to standard output.
    - For array input parse the array then pass it to the function.
    - Do not add extra print statement otherwise it will failed the test cases.
    """
```

### 2. LiveCodeBench ì „ìš© ìµœì í™”
```python
# LiveCodeBench ë°ì´í„°ì…‹ ê°ì§€
def is_livecodebench(self) -> bool:
    return self.dataset_type == 'livecodebench'

# LiveCodeBench ì „ìš© ê³„íš í”„ë¡¬í”„íŠ¸
if self.is_livecodebench():
    input_for_planning = [
        {
            "role": "user",
            "content": f"""You are a competitive programming expert tasked with generating an appropriate plan to solve a given LiveCodeBench problem using the **{self.language}** programming language.
            
            ## Problem
            {problem}
            
            **Expected Output:**
            Your response must be structured as follows:
            
            ### Problem Understanding
            - Think about the original problem. Develop an initial understanding about the problem.
            - Identify the problem type (array, string, graph, dynamic programming, etc.)
            - Note any constraints or edge cases
            
            ### Recall Example Problem
            Recall a relevant and distinct competitive programming problem (different from problem mentioned above) and
            - Describe it briefly
            - Identify the algorithm category (greedy, DP, graph, etc.)
            - Generate {self.language} code step by step to solve that problem
            - Discuss the algorithm to solve this problem
            - Finally generate a planning to solve that problem
            
            ### Algorithm to solve the original problem
            - Write down the algorithm that is well suited for the original problem
            - Give some tutorials about the algorithm for example:
                - How to approach this type of algorithm
                - Important things to consider
                - Time and space complexity analysis
            
            ### Plan
            - Write down a detailed, step-by-step plan to solve the **original problem**.
            - Include edge case handling
            - Consider optimization strategies
            
            --------
            **Important Instruction:**
            - Strictly follow the instructions.
            - Do not generate code.
            - Focus on competitive programming best practices."""
        },
    ]
```

## ğŸ”„ MapCoderì™€ì˜ ì°¨ì´ì 

### 1. Exemplar ì ‘ê·¼ë²• ì°¨ì´
- **MapCoder**: "multiple ungrounded exemplars" ì‚¬ìš©
- **CodeSIM**: "single exemplar" ê¸°ë°˜ ê³„íš ìƒì„±

### 2. ê³„íš ê²€ì¦ ë‹¨ê³„ ì¶”ê°€
```python
# CodeSIMì˜ ê³„íš ê²€ì¦ ë‹¨ê³„ (MapCoderì—ëŠ” ì—†ìŒ)
input_for_simulation = [
    {
        "role": "user",
        "content": prompt_for_simulation.format(
            problem_with_planning=problem_with_planning,
            language=self.language,
        )
    },
]

# ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ì— ë”°ë¥¸ ê³„íš ìˆ˜ì •
if "Plan Modification Needed" in response:
    input_for_plan_refinement = [
        {
            "role": "user",
            "content": prompt_for_plan_refinement.format(
                problem_with_planning=problem_with_planning,
                language=self.language,
                critique=response
            )
        },
    ]
```

### 3. ë‚´ë¶€ ë””ë²„ê¹… ë©”ì»¤ë‹ˆì¦˜
```python
# CodeSIMì˜ ì‹œë®¬ë ˆì´ì…˜ ê¸°ë°˜ ë‚´ë¶€ ë””ë²„ê¹…
prompt_for_debugging = """
### Simulation with failed test case
To detect where is the bug follow following steps:
    - Take a sample test case where it fails.
    - Take the input go through each step according to the plan
    - You will get a output that must be different from the expected output.

### Debugging Notes
- Based on this simulation detect any of the following cases:
    - Plan is wrong
    - Plan is correct but plan to code generation is wrong.
- Finally, discuss how to correct this code.
"""
```

## ğŸŒŸ í™•ì¥ì„± ë° ëª¨ë¸ ì—°ë™

### 1. ëª¨ë¸ íŒ©í† ë¦¬ íŒ¨í„´
```python
class ModelFactory:
    @staticmethod
    def get_model_class(model_provider_name: str):
        model_provider_name = model_provider_name.lower()
        if model_provider_name == "gemini":
            return Gemini
        elif model_provider_name == "openai":
            return OpenAIV1Model
        elif model_provider_name == "openai-v2":
            return OpenAIV2Model
        elif model_provider_name == "groq":
            return GroqModel
        elif model_provider_name == "anthropic":
            return AnthropicModel
```

### 2. ì „ëµ íŒ©í† ë¦¬ íŒ¨í„´
```python
class PromptingFactory:
    @staticmethod
    def get_prompting_class(strategy_name: str):
        if strategy_name == "CodeSIM":
            return CodeSIM
        elif strategy_name == "CodeSIMWPVD":
            return CodeSIMWPVD
        elif strategy_name == "CodeSIMWD":
            return CodeSIMWD
        elif strategy_name == "CodeSIMWPV":
            return CodeSIMWPV
        elif strategy_name == "CodeSIMA":
            return CodeSIMA
        elif strategy_name == "CodeSIMC":
            return CodeSIMC
```

### 3. ë°ì´í„°ì…‹ íŒ©í† ë¦¬ íŒ¨í„´
```python
class DatasetFactory:
    @staticmethod
    def create_dataset(dataset_name, **kwargs):
        dataset_class = DatasetFactory.get_dataset_class(dataset_name)
        
        if dataset_name.lower() in ["livecodebench", "lcb"] or dataset_name.startswith("lcb_"):
            if dataset_name.startswith("lcb_"):
                version = dataset_name.replace("lcb_", "")
            else:
                version = kwargs.get('release_version', 'release_v6')
            return dataset_class(release_version=version)
        else:
            return dataset_class(**kwargs)
```

## ğŸ“ˆ ê²°ê³¼ ë¶„ì„ ë° í•´ì„

### 1. ì‹¤í–‰ ê²°ê³¼ êµ¬ì¡°
```
results/
â””â”€â”€ {DATASET}/
    â””â”€â”€ {STRATEGY}/
        â””â”€â”€ {MODEL_NAME}/
            â””â”€â”€ {LANGUAGE}-{TEMPERATURE}-{TOP_P}-{PASS_AT_K}/
                â”œâ”€â”€ Run-{run_no}/
                â”‚   â”œâ”€â”€ Results.jsonl          # ê¸°ë³¸ ê²°ê³¼
                â”‚   â”œâ”€â”€ Summary.txt            # ìš”ì•½ í†µê³„
                â”‚   â”œâ”€â”€ Log.txt                # ì‹¤í–‰ ë¡œê·¸
                â”‚   â”œâ”€â”€ Results-ET.jsonl       # ET í‰ê°€ ê²°ê³¼
                â”‚   â”œâ”€â”€ Summary-ET.txt         # ET ìš”ì•½
                â”‚   â”œâ”€â”€ Results-EP.jsonl       # EP í‰ê°€ ê²°ê³¼
                â”‚   â”œâ”€â”€ Summary-EP.txt         # EP ìš”ì•½
                â”‚   â”œâ”€â”€ Results-LCB.jsonl      # LiveCodeBench ê²°ê³¼
                â”‚   â”œâ”€â”€ Summary-LCB.txt        # LiveCodeBench ìš”ì•½
                â”‚   â””â”€â”€ Report-LCB.json        # LiveCodeBench ìƒì„¸ ë¦¬í¬íŠ¸
```

### 2. ì„±ëŠ¥ ì§€í‘œ í•´ì„
- **Pass@1**: ì²« ë²ˆì§¸ ì‹œë„ì—ì„œ í†µê³¼í•œ ë¬¸ì œ ë¹„ìœ¨
- **ET (Execution Time)**: ì½”ë“œ ì‹¤í–‰ ì‹œê°„ ë¶„ì„
- **EP (Execution Pass)**: ì‹¤í–‰ í†µê³¼ìœ¨ ë¶„ì„
- **LCB (LiveCodeBench)**: ê²½ìŸ í”„ë¡œê·¸ë˜ë° íŠ¹í™” í‰ê°€

## ğŸ”® í–¥í›„ ë°œì „ ë°©í–¥

### 1. ëª¨ë¸ í™•ì¥
- **o3-mini**: Ollama ê¸°ë°˜ ë¡œì»¬ ëª¨ë¸ ì—°ë™
- **GPT-4o**: ìµœì‹  OpenAI ëª¨ë¸ ì§€ì›
- **Claude 3.5 Sonnet**: Anthropic ìµœì‹  ëª¨ë¸ ì§€ì›

### 2. ì „ëµ í™•ì¥
- **CodeSIM+**: ê°•í™”í•™ìŠµ ê¸°ë°˜ ì—ì´ì „íŠ¸ í˜‘ë ¥ ìµœì í™”
- **CodeSIM-Multi**: ë‹¤ì¤‘ ì–¸ì–´ ë™ì‹œ ìƒì„± ì§€ì›
- **CodeSIM-Adaptive**: ë¬¸ì œ ìœ í˜•ë³„ ìë™ ì „ëµ ì„ íƒ

### 3. í‰ê°€ ì‹œìŠ¤í…œ í™•ì¥
- **Code Quality Metrics**: ì½”ë“œ í’ˆì§ˆ ì§€í‘œ ì¶”ê°€
- **Runtime Performance**: ì‹¤í–‰ ì‹œê°„ ì„±ëŠ¥ ë¶„ì„
- **Memory Usage**: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„

## ğŸ“š ì°¸ê³  ìë£Œ

- **ë…¼ë¬¸**: CODESIM: Multi-Agent Code Generation with Simulation-Driven Planning and Debugging (NAACL 2025 Findings)
- **ì½”ë“œë² ì´ìŠ¤**: [GitHub Repository](https://github.com/your-repo/codesim)
- **ë°ì´í„°ì…‹**: HumanEval, MBPP, APPS, CodeContests, LiveCodeBench
- **í‰ê°€ í”„ë ˆì„ì›Œí¬**: ExecEval, Pass@k metrics

---

**Note**: ì´ READMEëŠ” CODESIM í”„ë ˆì„ì›Œí¬ì˜ ì‹¤ì œ êµ¬í˜„ ì½”ë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìœ¼ë©°, ë…¼ë¬¸ì—ì„œ ì œì‹œí•œ ì´ë¡ ì  ê°œë…ë“¤ì´ ì–´ë–»ê²Œ ì‹¤ì œë¡œ êµ¬í˜„ë˜ì—ˆëŠ”ì§€ ìƒì„¸íˆ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤.
