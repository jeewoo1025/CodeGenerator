# CodeGenerator

CodeGeneratorλ” λ‹¤μ–‘ν• λ€κ·λ¨ μ–Έμ–΄ λ¨λΈ(LLM)μ„ μ‚¬μ©ν•μ—¬ μ½”λ“ μƒμ„± μ„±λ¥μ„ ν‰κ°€ν•λ” ν”„λ μ„μ›ν¬μ…λ‹λ‹¤. OpenAI, Anthropic, Google, vLLM λ“± λ‹¤μ–‘ν• λ¨λΈμ„ μ§€μ›ν•λ©°, Direct, CoT, CodeSIM, MapCoder λ“± λ‹¤μ–‘ν• ν”„λ΅¬ν”„ν… μ „λµμ„ ν†µν•΄ μ½”λ“ μƒμ„± μ„±λ¥μ„ ν‰κ°€ν•  μ μμµλ‹λ‹¤.

## π€ μ£Όμ” κΈ°λ¥

- **λ‹¤μ–‘ν• λ¨λΈ μ§€μ›**: OpenAI, Anthropic, Google, vLLM λ“±
- **Qwen3 λ¨λΈ μ§€μ›**: Qwen3.5, Qwen3, Qwen3-Coder κ³„μ—΄ λ¨λΈ (μ΄ 43κ° λ¨λΈ)
- **λ‹¤μ–‘ν• μ „λµ**: Direct, CoT, CodeSIM, MapCoder, SelfPlanning, Analogical λ“±
- **λ‹¤μ–‘ν• λ°μ΄ν„°μ…‹**: HumanEval, MBPP, LiveCodeBench, APPS, xCodeEval λ“±
- **μ‹¤μ‹κ°„ ν‰κ°€**: μ½”λ“ μ‹¤ν–‰ λ° ν…μ¤νΈ μλ™ν™”
- **ν¬λ΅μ¤ ν”λ«νΌ**: Windows, Linux, macOS μ§€μ›

## π“¦ μ„¤μΉ

### 1. κΈ°λ³Έ μμ΅΄μ„± μ„¤μΉ

```bash
# μμ΅΄μ„± μ„¤μΉ
pip install -r requirements.txt
```

### 2. vLLM μ„¤μΉ ν™•μΈ

vLLMμ΄ μ λ€λ΅ μ„¤μΉλμ—λ”μ§€ ν™•μΈ:

```bash
python -c "import vllm; print('vLLM μ„¤μΉ μ™„λ£')"
```

### 3. CUDA μ„¤μ • ν™•μΈ

```bash
# CUDA λ²„μ „ ν™•μΈ
nvidia-smi
nvcc --version

# PyTorch CUDA μ§€μ› ν™•μΈ
python -c "import torch; print(f'CUDA μ‚¬μ© κ°€λ¥: {torch.cuda.is_available()}')"
```

## π― μ‚¬μ©λ²•

### 1. Qwen3 λ¨λΈ ν‰κ°€ (vLLM) - κ¶μ¥

#### κΈ°λ³Έ μ‚¬μ©λ²•
```bash
# Direct μ „λµμΌλ΅ Qwen3-Coder-7B ν‰κ°€
python run_qwen_evaluation.py \
    --model Qwen3-Coder-7B \
    --dataset HumanEval \
    --strategy Direct
```

#### CodeSIM μ „λµ μ‚¬μ©
```bash
# CodeSIM μ „λµμΌλ΅ ν‰κ°€ (μ½”λ“ μ „μ© λ¨λΈ κ¶μ¥)
python run_qwen_evaluation.py \
    --model Qwen3-Coder-7B \
    --dataset HumanEval \
    --strategy CodeSIM \
    --max_plan_try 5 \
    --max_debug_try 5 \
    --additional_info_run 0
```

#### λ‹¤μ–‘ν• μ „λµ μ‚¬μ©
```bash
# MapCoder μ „λµ
python run_qwen_evaluation.py \
    --model Qwen3-Coder-14B \
    --dataset HumanEval \
    --strategy MapCoder

    # CoT (Chain of Thought) μ „λµ
    python run_qwen_evaluation.py \
        --model Qwen3-7B \
        --dataset HumanEval \
        --strategy CoT

    # SelfPlanning μ „λµ
    python run_qwen_evaluation.py \
        --model Qwen3-14B \
        --dataset HumanEval \
        --strategy SelfPlanning
```

#### LiveCodeBench λ°μ΄ν„°μ…‹
```bash
# LiveCodeBench λ°μ΄ν„°μ…‹μΌλ΅ ν‰κ°€
python run_qwen_evaluation.py \
    --model Qwen3-Coder-14B \
    --dataset LiveCodeBench \
    --strategy CodeSIM \
    --temperature 0.1 \
    --max_tokens 4096 \
    --tensor_parallel_size 2
```

### 2. κΈ°μ΅΄ main.py μ‚¬μ©

```bash
# vLLMμΌλ΅ Qwen3-Coder-7B ν‰κ°€
python src/main.py \
    --model Qwen3-Coder-7B \
    --model_provider vllm \
    --dataset HumanEval \
    --strategy Direct \
    --temperature 0 \
    --top_p 0.95

# CodeSIM μ „λµ μ‚¬μ©
python src/main.py \
    --model Qwen3-Coder-7B \
    --model_provider vllm \
    --dataset HumanEval \
    --strategy CodeSIM \
    --temperature 0 \
    --top_p 0.95
```

### 3. λ‹¤λ¥Έ λ¨λΈλ“¤

```bash
# OpenAI GPT-4
python src/main.py \
    --model gpt-4 \
    --model_provider OpenAI \
    --dataset HumanEval \
    --strategy Direct

# Anthropic Claude-3-Sonnet
python src/main.py \
    --model claude-3-sonnet \
    --model_provider anthropic \
    --dataset HumanEval \
    --strategy Direct

# Google Gemini Pro
python src/main.py \
    --model gemini-pro \
    --model_provider Google \
    --dataset HumanEval \
    --strategy Direct
```

## π—οΈ μ§€μ› λ¨λΈ

### Qwen3 κ³„μ—΄ (vLLM)

#### Qwen3.5 κ³„μ—΄
- `Qwen3.5-0.5B` - 0.5B νλΌλ―Έν„°
- `Qwen3.5-1.8B` - 1.8B νλΌλ―Έν„°  
- `Qwen3.5-4B` - 4B νλΌλ―Έν„°
- `Qwen3.5-7B` - 7B νλΌλ―Έν„°
- `Qwen3.5-14B` - 14B νλΌλ―Έν„°
- `Qwen3.5-32B` - 32B νλΌλ―Έν„°
- `Qwen3.5-72B` - 72B νλΌλ―Έν„°

#### Qwen3 κ³„μ—΄
- `Qwen3-0.5B` - 0.5B νλΌλ―Έν„°
- `Qwen3-1.5B` - 1.5B νλΌλ―Έν„°
- `Qwen3-3B` - 3B νλΌλ―Έν„°
- `Qwen3-7B` - 7B νλΌλ―Έν„°
- `Qwen3-14B` - 14B νλΌλ―Έν„°
- `Qwen3-32B` - 32B νλΌλ―Έν„°
- `Qwen3-72B` - 72B νλΌλ―Έν„°

#### Qwen3.5-MoE κ³„μ—΄
- `Qwen3.5-MoE-2.7B` - 2.7B νλΌλ―Έν„°
- `Qwen3.5-MoE-3.5B` - 3.5B νλΌλ―Έν„°
- `Qwen3.5-MoE-6.5B` - 6.5B νλΌλ―Έν„°
- `Qwen3.5-MoE-12B` - 12B νλΌλ―Έν„°
- `Qwen3.5-MoE-20B` - 20B νλΌλ―Έν„°
- `Qwen3.5-MoE-32B` - 32B νλΌλ―Έν„°

#### π†• Qwen3-Coder κ³„μ—΄ (μ½”λ“ μ „μ© λ¨λΈ)
- `Qwen3-Coder-0.5B` - 0.5B νλΌλ―Έν„°
- `Qwen3-Coder-1.5B` - 1.5B νλΌλ―Έν„°
- `Qwen3-Coder-3B` - 3B νλΌλ―Έν„°
- `Qwen3-Coder-7B` - 7B νλΌλ―Έν„°
- `Qwen3-Coder-14B` - 14B νλΌλ―Έν„°
- `Qwen3-Coder-32B` - 32B νλΌλ―Έν„°
- `Qwen3-Coder-72B` - 72B νλΌλ―Έν„°

#### Qwen3-Coder-MoE κ³„μ—΄
- `Qwen3-Coder-MoE-2.7B` - 2.7B νλΌλ―Έν„°
- `Qwen3-Coder-MoE-3.5B` - 3.5B νλΌλ―Έν„°
- `Qwen3-Coder-MoE-6.5B` - 6.5B νλΌλ―Έν„°
- `Qwen3-Coder-MoE-12B` - 12B νλΌλ―Έν„°
- `Qwen3-Coder-MoE-20B` - 20B νλΌλ―Έν„°
- `Qwen3-Coder-MoE-32B` - 32B νλΌλ―Έν„°

### κΈ°νƒ€ λ¨λΈ
- **OpenAI**: GPT-3.5-turbo, GPT-4, GPT-4o, GPT-4o-mini λ“±
- **Anthropic**: Claude-3-Haiku, Claude-3-Sonnet, Claude-3-Opus λ“±
- **Google**: Gemini Pro, Gemini Flash, Gemini 1.5 Pro λ“±
- **Groq**: Llama3-8B, Llama3-70B, Mixtral-8x7B λ“±

## π­ μ§€μ› μ „λµ

### κΈ°λ³Έ μ „λµ
- **Direct**: μ§μ ‘ μ½”λ“ μƒμ„± (κ°€μ¥ λΉ λ¥΄κ³  ν¨μ¨μ )
- **CoT**: Chain of Thought (λ‹¨κ³„λ³„ μ‚¬κ³  κ³Όμ •)
- **SelfPlanning**: μμ²΄ κ³„ν μλ¦½ λ° μ‹¤ν–‰

### κ³ κΈ‰ μ „λµ
- **CodeSIM**: μ½”λ“ μ‹λ®¬λ μ΄μ…, κ³„ν μλ¦½, λ””λ²„κΉ… (κ°€μ¥ μ •ν™•ν•¨)
- **MapCoder**: λ§µν•‘ κΈ°λ° μ½”λ“ μƒμ„±
- **Analogical**: μ μ‚¬ μ‚¬λ΅€ κΈ°λ° μƒμ„±

### CodeSIM λ³€ν• μ „λµ
- **CodeSIMWD**: CodeSIM with Debugging
- **CodeSIMWPV**: CodeSIM with Planning and Validation
- **CodeSIMWPVD**: CodeSIM with Planning, Validation and Debugging
- **CodeSIMA**: CodeSIM Advanced
- **CodeSIMC**: CodeSIM Compact

## π“ μ§€μ› λ°μ΄ν„°μ…‹

### μ½”λ“ μƒμ„± λ°μ΄ν„°μ…‹
- **HumanEval**: Python ν•¨μ μƒμ„± (164λ¬Έμ )
- **MBPP**: Python ν”„λ΅κ·Έλλ° λ¬Έμ  (974λ¬Έμ )
- **APPS**: ν”„λ΅κ·Έλλ° λ¬Έμ  ν’€μ΄ (10,000λ¬Έμ )

### μ‹¤μ‹κ°„ μ‹¤ν–‰ λ°μ΄ν„°μ…‹
- **LiveCodeBench**: μ‹¤μ‹κ°„ μ½”λ“ μ‹¤ν–‰ ν‰κ°€ (μµμ‹  v6 μ§€μ›)
- **xCodeEval**: λ‹¤μ–‘ν• μ–Έμ–΄ μ½”λ“ μƒμ„±

### κ²½μ ν”„λ΅κ·Έλλ°
- **CodeContest**: Google Code Jam μ¤νƒ€μΌ λ¬Έμ 

## π”§ μ‹μ¤ν… μ”κµ¬μ‚¬ν•­

### GPU λ©”λ¨λ¦¬ μ”κµ¬μ‚¬ν•­

| λ¨λΈ ν¬κΈ° | μµμ† GPU λ©”λ¨λ¦¬ | κ¶μ¥ GPU λ©”λ¨λ¦¬ | κ¶μ¥ GPU |
|-----------|----------------|----------------|----------|
| 0.5B-1.8B | 4GB | 8GB | RTX 3060, RTX 4060 |
| 4B-7B | 8GB | 16GB | RTX 3070, RTX 4070 |
| 14B-32B | 16GB | 32GB | RTX 3090, RTX 4090 |
| 72B | 32GB | 64GB+ | A100, H100 |

### κ¶μ¥ ν•λ“μ›¨μ–΄

- **GPU**: NVIDIA RTX 3090, RTX 4090, A100, H100
- **RAM**: 32GB μ΄μƒ (72B λ¨λΈμ κ²½μ° 64GB+)
- **Storage**: SSD (λ¨λΈ λ‹¤μ΄λ΅λ“μ©, μµμ† 100GB μ—¬μ  κ³µκ°„)
- **CPU**: 8μ½”μ–΄ μ΄μƒ (Intel i7/Ryzen 7 μ΄μƒ)

### μ†ν”„νΈμ›¨μ–΄ μ”κµ¬μ‚¬ν•­

- **OS**: Windows 10/11, Ubuntu 18.04+, macOS 10.15+
- **Python**: 3.8 μ΄μƒ (3.9+ κ¶μ¥)
- **CUDA**: 11.8 μ΄μƒ (12.0+ κ¶μ¥)
- **PyTorch**: 2.0 μ΄μƒ

## π“ ν”„λ΅μ νΈ κµ¬μ΅°

```
CodeGenerator/
β”β”€β”€ src/                          # μ†μ¤ μ½”λ“
β”‚   β”β”€β”€ models/                   # λ¨λΈ κµ¬ν„
β”‚   β”‚   β”β”€β”€ Base.py              # κΈ°λ³Έ λ¨λΈ ν΄λμ¤
β”‚   β”‚   β”β”€β”€ OpenAI.py            # OpenAI λ¨λΈ
β”‚   β”‚   β”β”€β”€ Anthropic.py         # Anthropic λ¨λΈ
β”‚   β”‚   β”β”€β”€ VLLMModel.py         # vLLM λ¨λΈ (Qwen3 μ§€μ›)
β”‚   β”‚   β”β”€β”€ Gemini.py            # Google Gemini λ¨λΈ
β”‚   β”‚   β””β”€β”€ ModelFactory.py      # λ¨λΈ ν©ν† λ¦¬
β”‚   β”β”€β”€ promptings/               # ν”„λ΅¬ν”„ν… μ „λµ
β”‚   β”‚   β”β”€β”€ Base.py              # κΈ°λ³Έ μ „λµ ν΄λμ¤
β”‚   β”‚   β”β”€β”€ Direct.py            # Direct μ „λµ
β”‚   β”‚   β”β”€β”€ CodeSIM.py           # CodeSIM μ „λµ
β”‚   β”‚   β”β”€β”€ MapCoder.py          # MapCoder μ „λµ
β”‚   β”‚   β””β”€β”€ PromptingFactory.py  # μ „λµ ν©ν† λ¦¬
β”‚   β”β”€β”€ datasets/                 # λ°μ΄ν„°μ…‹ λ΅λ”
β”‚   β”‚   β”β”€β”€ HumanEvalDataset.py  # HumanEval λ°μ΄ν„°μ…‹
β”‚   β”‚   β”β”€β”€ LiveCodeBenchDataset.py # LiveCodeBench λ°μ΄ν„°μ…‹
β”‚   β”‚   β””β”€β”€ DatasetFactory.py    # λ°μ΄ν„°μ…‹ ν©ν† λ¦¬
β”‚   β”β”€β”€ evaluations/              # ν‰κ°€ λ΅μ§
β”‚   β”‚   β”β”€β”€ func_evaluate.py     # ν•¨μ ν‰κ°€
β”‚   β”‚   β””β”€β”€ resource_limit.py    # λ¦¬μ†μ¤ μ ν•
β”‚   β”β”€β”€ constants/                # μƒμ μ •μ
β”‚   β”‚   β”β”€β”€ qwen_models.py       # Qwen3 λ¨λΈ μ„¤μ •
β”‚   β”‚   β””β”€β”€ paths.py             # κ²½λ΅ μƒμ
β”‚   β”β”€β”€ utils/                    # μ ν‹Έλ¦¬ν‹° ν•¨μ
β”‚   β”‚   β”β”€β”€ summary.py           # κ²°κ³Ό μ”μ•½
β”‚   β”‚   β””β”€β”€ parse.py             # νμ‹± μ ν‹Έλ¦¬ν‹°
β”‚   β”β”€β”€ results/                  # κ²°κ³Ό μ²λ¦¬
β”‚   β”‚   β””β”€β”€ Results.py           # κ²°κ³Ό ν΄λμ¤
β”‚   β””β”€β”€ main.py                  # λ©”μΈ μ‹¤ν–‰ μ¤ν¬λ¦½νΈ
β”β”€β”€ data/                         # λ°μ΄ν„°μ…‹ νμΌλ“¤
β”‚   β”β”€β”€ HumanEval/               # HumanEval λ°μ΄ν„°
β”‚   β”β”€β”€ MBPP/                    # MBPP λ°μ΄ν„°
β”‚   β”β”€β”€ LiveCodeBench/           # LiveCodeBench λ°μ΄ν„°
β”‚   β””β”€β”€ APPS/                    # APPS λ°μ΄ν„°
β”β”€β”€ results/                      # ν‰κ°€ κ²°κ³Ό (μλ™ μƒμ„±)
β”β”€β”€ run_qwen_evaluation.py       # Qwen3 λ¨λΈ ν‰κ°€ ν†µν•© μ¤ν¬λ¦½νΈ
β”β”€β”€ test_setup.py                # μ„¤μ • ν…μ¤νΈ μ¤ν¬λ¦½νΈ
β”β”€β”€ requirements.txt              # Python μμ΅΄μ„±
β””β”€β”€ README.md                     # μ΄ νμΌ
```

## π€ λΉ λ¥Έ μ‹μ‘

### 1. μ„¤μΉ λ° μ„¤μ •

```bash
# μ €μ¥μ† ν΄λ΅ 
git clone https://github.com/your-username/CodeGenerator.git
cd CodeGenerator

# μμ΅΄μ„± μ„¤μΉ
pip install -r requirements.txt

# μ„¤μ • ν…μ¤νΈ
python test_setup.py
```

### 2. μ²« λ²μ§Έ ν‰κ°€ μ‹¤ν–‰

```bash
# Qwen3-Coder-7Bλ΅ HumanEval ν‰κ°€
python run_qwen_evaluation.py \
    --model Qwen3-Coder-7B \
    --dataset HumanEval \
    --strategy Direct
```

### 3. κ²°κ³Ό ν™•μΈ

```bash
# κ²°κ³Ό λ””λ ‰ν† λ¦¬ ν™•μΈ
ls results/

# μ”μ•½ νμΌ ν™•μΈ
cat results/Qwen_Qwen3-Coder-7B_HumanEval_Direct_*/Summary.txt
```

## π― κ³ κΈ‰ μ‚¬μ©λ²•

### λ°°μΉ ν‰κ°€

μ—¬λ¬ λ¨λΈμ„ μμ°¨μ μΌλ΅ ν‰κ°€:

```bash
    # μ—¬λ¬ λ¨λΈ ν‰κ°€
    for model in "Qwen3-Coder-3B" "Qwen3-Coder-7B" "Qwen3-Coder-14B"; do
    python run_qwen_evaluation.py \
        --model $model \
        --dataset HumanEval \
        --strategy Direct
done
```

### λ‹¤μ–‘ν• μ „λµ λΉ„κµ

```bash
# κ°™μ€ λ¨λΈλ΅ λ‹¤λ¥Έ μ „λµ λΉ„κµ
for strategy in "Direct" "CoT" "CodeSIM" "MapCoder"; do
    python run_qwen_evaluation.py \
        --model Qwen3-Coder-7B \
        --dataset HumanEval \
        --strategy $strategy
done
```

### μ„±λ¥ μµμ ν™”

```bash
# GPU λ©”λ¨λ¦¬ μµμ ν™”
python run_qwen_evaluation.py \
    --model Qwen3-Coder-7B \
    --dataset HumanEval \
    --strategy Direct \
    --gpu_memory_utilization 0.8 \
    --max_tokens 1024

# λ‹¤μ¤‘ GPU μ‚¬μ©
python run_qwen_evaluation.py \
    --model Qwen3-Coder-14B \
    --dataset HumanEval \
    --strategy Direct \
    --tensor_parallel_size 2
```

## π” λ¬Έμ  ν•΄κ²°

### μΌλ°μ μΈ λ¬Έμ λ“¤

#### 1. GPU λ©”λ¨λ¦¬ λ¶€μ΅±
```bash
# GPU λ©”λ¨λ¦¬ μ‚¬μ©λ¥  μ΅°μ •
--gpu_memory_utilization 0.7

    # λ” μ‘μ€ λ¨λΈ μ‚¬μ©
    --model Qwen3-Coder-3B

# μµλ€ ν† ν° μ μ¤„μ΄κΈ°
--max_tokens 1024
```

#### 2. λ¨λΈ λ‹¤μ΄λ΅λ“ μ‹¤ν¨
```bash
# Hugging Face ν† ν° μ„¤μ •
export HF_TOKEN=your_token_here

# λ„¤νΈμ›ν¬ νƒ€μ„μ•„μ›ƒ μ¦κ°€
export HF_HUB_DOWNLOAD_TIMEOUT=1000
```

#### 3. vLLM μ΄κΈ°ν™” μ‹¤ν¨
```bash
# CUDA λ²„μ „ ν™•μΈ
nvidia-smi
nvcc --version

# vLLM μ¬μ„¤μΉ
pip uninstall vllm
pip install vllm

# PyTorch μ¬μ„¤μΉ
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

#### 4. CodeSIM μ‹¤ν–‰ μ¤λ¥
```bash
# λ” μ‘μ€ λ°°μΉ ν¬κΈ°
--max_tokens 1024

# λ©”λ¨λ¦¬ μ‚¬μ©λ‰ μ¤„μ΄κΈ°
--gpu_memory_utilization 0.7
```

### μ„±λ¥ μµμ ν™” ν

#### 1. λ¨λΈ μ„ νƒ κ°€μ΄λ“
- **μΌλ° μ©λ„**: Qwen3-7B (κ· ν•μ΅ν μ„±λ¥)
- **μ½”λ“ μƒμ„±**: Qwen3-Coder-7B (μµμ ν™”λ μ„±λ¥)
- **μ ν•λ λ¦¬μ†μ¤**: Qwen3-Coder-3B (8GB GPU)
- **μµκ³  μ„±λ¥**: Qwen3-Coder-14B (16GB+ GPU)

#### 2. μ „λµ μ„ νƒ κ°€μ΄λ“
- **λΉ λ¥Έ ν‰κ°€**: Direct (κ°€μ¥ λΉ λ¦„)
- **μ •ν™•ν• ν‰κ°€**: CodeSIM (κ°€μ¥ μ •ν™•ν•¨)
- **κ· ν•μ΅ν**: CoT (μ†λ„μ™€ μ •ν™•λ„ κ· ν•)
- **κ³ κΈ‰ λ¶„μ„**: MapCoder (λ³µμ΅ν• λ¬Έμ )

#### 3. ν•λ“μ›¨μ–΄ μµμ ν™”
- **λ‹¨μΌ GPU**: tensor_parallel_size=1
- **λ‹¤μ¤‘ GPU**: tensor_parallel_size=2 (λλ” 4)
- **λ©”λ¨λ¦¬ μµμ ν™”**: gpu_memory_utilization=0.8
- **λ°°μΉ μ²λ¦¬**: max_tokens=2048

## π“ κ²°κ³Ό λ¶„μ„

### κ²°κ³Ό νμΌ κµ¬μ΅°

```
results/
β””β”€β”€ Qwen_Qwen3-Coder-7B_HumanEval_Direct_20241201_143022/
    β”β”€β”€ Results.jsonl          # μƒμ„Έ ν‰κ°€ κ²°κ³Ό
    β”β”€β”€ Summary.txt            # κ²°κ³Ό μ”μ•½
    β”β”€β”€ Log.txt               # μ‹¤ν–‰ λ΅κ·Έ
    β”β”€β”€ Results-ET.jsonl      # Execution Time κ²°κ³Ό
    β””β”€β”€ Summary-ET.txt        # Execution Time μ”μ•½
```

### κ²°κ³Ό ν•΄μ„

#### Pass@k μ§€ν‘
- **Pass@1**: μ²« λ²μ§Έ μ‹λ„μ—μ„ ν†µκ³Όν• λΉ„μ¨
- **Pass@10**: 10λ² μ‹λ„ μ¤‘ ν†µκ³Όν• λΉ„μ¨
- **Pass@100**: 100λ² μ‹λ„ μ¤‘ ν†µκ³Όν• λΉ„μ¨

#### μ‹¤ν–‰ μ‹κ°„ λ¶„μ„
- **ν‰κ·  μ‹¤ν–‰ μ‹κ°„**: λ¨λ“  ν…μ¤νΈ μΌ€μ΄μ¤μ ν‰κ· 
- **μµλ€ μ‹¤ν–‰ μ‹κ°„**: κ°€μ¥ μ¤λ κ±Έλ¦° ν…μ¤νΈ μΌ€μ΄μ¤
- **λ©”λ¨λ¦¬ μ‚¬μ©λ‰**: GPU λ° μ‹μ¤ν… λ©”λ¨λ¦¬ μ‚¬μ©λ‰

## π”® ν–¥ν›„ κ³„ν

### λ‹¨κΈ° κ³„ν (1-3κ°μ›”)
- [ ] λ” λ§μ€ Qwen3 λ¨λΈ μ§€μ›
- [ ] μƒλ΅μ΄ ν”„λ΅¬ν”„ν… μ „λµ μ¶”κ°€
- [ ] μ„±λ¥ λ²¤μΉλ§ν¬ κ°μ„ 

### μ¤‘κΈ° κ³„ν (3-6κ°μ›”)
- [ ] μ›Ή μΈν„°νμ΄μ¤ κ°λ°
- [ ] λ¶„μ‚° ν‰κ°€ μ‹μ¤ν… κµ¬μ¶•
- [ ] μ‹¤μ‹κ°„ λ¨λ‹ν„°λ§ λ€μ‹λ³΄λ“

### μ¥κΈ° κ³„ν (6κ°μ›”+)
- [ ] ν΄λΌμ°λ“ λ°°ν¬ μ§€μ›
- [ ] μλ™ ν•μ΄νΌνλΌλ―Έν„° νλ‹
- [ ] λ©€ν‹° λ¨λ‹¬ ν‰κ°€ μ§€μ›

## π¤ κΈ°μ—¬ν•κΈ°

### λ²„κ·Έ λ¦¬ν¬νΈ
- GitHub Issuesλ¥Ό ν†µν•΄ λ²„κ·Έλ¥Ό λ¦¬ν¬νΈν•΄μ£Όμ„Έμ”
- μ¬ν„ κ°€λ¥ν• μµμ†ν•μ μμ λ¥Ό ν¬ν•¨ν•΄μ£Όμ„Έμ”

### κΈ°λ¥ μ μ•
- μƒλ΅μ΄ κΈ°λ¥μ΄λ‚ κ°μ„ μ‚¬ν•­μ„ μ μ•ν•΄μ£Όμ„Έμ”
- κµ¬μ²΄μ μΈ μ‚¬μ© μ‚¬λ΅€λ¥Ό μ„¤λ…ν•΄μ£Όμ„Έμ”

### μ½”λ“ κΈ°μ—¬
- Fork ν›„ Pull Requestλ¥Ό λ³΄λ‚΄μ£Όμ„Έμ”
- μ½”λ“ μ¤νƒ€μΌ κ°€μ΄λ“λ¥Ό λ”°λΌμ£Όμ„Έμ”

## π“ μ°Έκ³  μλ£

### κ³µμ‹ λ¬Έμ„
- [vLLM κ³µμ‹ λ¬Έμ„](https://docs.vllm.ai/)
- [Qwen λ¨λΈ ν—λΈ](https://huggingface.co/Qwen)
- [Qwen3-Coder GitHub](https://github.com/QwenLM/Qwen3-Coder)

### κ΄€λ ¨ λ…Όλ¬Έ
- [Qwen3 Technical Report](https://arxiv.org/abs/2505.09388)
- [Qwen2.5-Coder Technical Report](https://arxiv.org/abs/2409.12186)

### μ»¤λ®¤λ‹ν‹°
- [Qwen Discord](https://discord.gg/qwen)
- [Hugging Face Forums](https://discuss.huggingface.co/)

## π“„ λΌμ΄μ„ μ¤

μ΄ ν”„λ΅μ νΈλ” MIT λΌμ΄μ„ μ¤ ν•μ— λ°°ν¬λ©λ‹λ‹¤. μμ„Έν• λ‚΄μ©μ€ [LICENSE](LICENSE) νμΌμ„ μ°Έμ΅°ν•μ„Έμ”.

## π™ κ°μ‚¬μ λ§

- [Qwen Team](https://github.com/QwenLM) - ν›λ¥­ν• λ¨λΈλ“¤μ„ μ κ³µν•΄μ£Όμ…”μ„ κ°μ‚¬ν•©λ‹λ‹¤
- [vLLM Team](https://github.com/vllm-project/vllm) - κ³ μ„±λ¥ μ¶”λ΅  μ—”μ§„μ„ μ κ³µν•΄μ£Όμ…”μ„ κ°μ‚¬ν•©λ‹λ‹¤
- [Hugging Face](https://huggingface.co/) - λ¨λΈ ν—λΈμ™€ λ„κµ¬λ“¤μ„ μ κ³µν•΄μ£Όμ…”μ„ κ°μ‚¬ν•©λ‹λ‹¤

---

**CodeGenerator**λ΅ λ” λ‚μ€ μ½”λ“ μƒμ„± AIλ¥Ό λ§λ“¤μ–΄κ°€μ”! π€β¨
